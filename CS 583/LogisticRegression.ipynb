{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZKkgyIotRUB"
   },
   "source": [
    "# HM1: Logistic Regression.\n",
    "\n",
    "### Name: Christian Bautista\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWcXk-YttRUD"
   },
   "source": [
    "#### For this assignment, you will build 6 models. You need to train Logistic Regression/Regularized Logistic Regression each with Batch Gradient Descent, Stochastic Gradient Descent and Mini Batch Gradient Descent. Also you should plot their objective values versus epochs and compare their training and testing accuracies. You will need to tune the parameters a little bit to obtain reasonable results.\n",
    "\n",
    "#### You do not have to follow the following procedure. You may implement your own functions and methods, but you need to show your results and plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "14FELOW9tRUE"
   },
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy\n",
    "from numpy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irEE0C8itRUF"
   },
   "source": [
    "# 1. Data processing\n",
    "\n",
    "- Download the Breast Cancer dataset from canvas or from https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)\n",
    "- Load the data.\n",
    "- Preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpNofJgdtRUG"
   },
   "source": [
    "## 1.1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "08YWBd5_tRUG",
    "outputId": "49656ec2-51d1-4be0-9487-766abfb9fc2b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0      842302         M        17.99         10.38          122.80     1001.0   \n",
       "1      842517         M        20.57         17.77          132.90     1326.0   \n",
       "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3    84348301         M        11.42         20.38           77.58      386.1   \n",
       "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
       "..        ...       ...          ...           ...             ...        ...   \n",
       "564    926424         M        21.56         22.39          142.00     1479.0   \n",
       "565    926682         M        20.13         28.25          131.20     1261.0   \n",
       "566    926954         M        16.60         28.08          108.30      858.1   \n",
       "567    927241         M        20.60         29.33          140.10     1265.0   \n",
       "568     92751         B         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0    ...          17.33           184.60      2019.0           0.16220   \n",
       "1    ...          23.41           158.80      1956.0           0.12380   \n",
       "2    ...          25.53           152.50      1709.0           0.14440   \n",
       "3    ...          26.50            98.87       567.7           0.20980   \n",
       "4    ...          16.67           152.20      1575.0           0.13740   \n",
       "..   ...            ...              ...         ...               ...   \n",
       "564  ...          26.40           166.10      2027.0           0.14100   \n",
       "565  ...          38.25           155.00      1731.0           0.11660   \n",
       "566  ...          34.12           126.70      1124.0           0.11390   \n",
       "567  ...          39.42           184.60      1821.0           0.16500   \n",
       "568  ...          30.37            59.16       268.6           0.08996   \n",
       "\n",
       "     compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0              0.66560           0.7119                0.2654          0.4601   \n",
       "1              0.18660           0.2416                0.1860          0.2750   \n",
       "2              0.42450           0.4504                0.2430          0.3613   \n",
       "3              0.86630           0.6869                0.2575          0.6638   \n",
       "4              0.20500           0.4000                0.1625          0.2364   \n",
       "..                 ...              ...                   ...             ...   \n",
       "564            0.21130           0.4107                0.2216          0.2060   \n",
       "565            0.19220           0.3215                0.1628          0.2572   \n",
       "566            0.30940           0.3403                0.1418          0.2218   \n",
       "567            0.86810           0.9387                0.2650          0.4087   \n",
       "568            0.06444           0.0000                0.0000          0.2871   \n",
       "\n",
       "     fractal_dimension_worst  Unnamed: 32  \n",
       "0                    0.11890          NaN  \n",
       "1                    0.08902          NaN  \n",
       "2                    0.08758          NaN  \n",
       "3                    0.17300          NaN  \n",
       "4                    0.07678          NaN  \n",
       "..                       ...          ...  \n",
       "564                  0.07115          NaN  \n",
       "565                  0.06637          NaN  \n",
       "566                  0.07820          NaN  \n",
       "567                  0.12400          NaN  \n",
       "568                  0.07039          NaN  \n",
       "\n",
       "[569 rows x 33 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data.csv\")\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-NuKVdCtRUH"
   },
   "source": [
    "## 1.2 Examine and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "3HLphKvotRUH",
    "outputId": "1456254f-1681-4614-f582-278d6e419137"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>-1</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>-1</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>-1</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>-1</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>1</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0           -1        17.99         10.38          122.80     1001.0   \n",
       "1           -1        20.57         17.77          132.90     1326.0   \n",
       "2           -1        19.69         21.25          130.00     1203.0   \n",
       "3           -1        11.42         20.38           77.58      386.1   \n",
       "4           -1        20.29         14.34          135.10     1297.0   \n",
       "..         ...          ...           ...             ...        ...   \n",
       "564         -1        21.56         22.39          142.00     1479.0   \n",
       "565         -1        20.13         28.25          131.20     1261.0   \n",
       "566         -1        16.60         28.08          108.30      858.1   \n",
       "567         -1        20.60         29.33          140.10     1265.0   \n",
       "568          1         7.76         24.54           47.92      181.0   \n",
       "\n",
       "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0            0.11840           0.27760         0.30010              0.14710   \n",
       "1            0.08474           0.07864         0.08690              0.07017   \n",
       "2            0.10960           0.15990         0.19740              0.12790   \n",
       "3            0.14250           0.28390         0.24140              0.10520   \n",
       "4            0.10030           0.13280         0.19800              0.10430   \n",
       "..               ...               ...             ...                  ...   \n",
       "564          0.11100           0.11590         0.24390              0.13890   \n",
       "565          0.09780           0.10340         0.14400              0.09791   \n",
       "566          0.08455           0.10230         0.09251              0.05302   \n",
       "567          0.11780           0.27700         0.35140              0.15200   \n",
       "568          0.05263           0.04362         0.00000              0.00000   \n",
       "\n",
       "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           0.2419  ...        25.380          17.33           184.60   \n",
       "1           0.1812  ...        24.990          23.41           158.80   \n",
       "2           0.2069  ...        23.570          25.53           152.50   \n",
       "3           0.2597  ...        14.910          26.50            98.87   \n",
       "4           0.1809  ...        22.540          16.67           152.20   \n",
       "..             ...  ...           ...            ...              ...   \n",
       "564         0.1726  ...        25.450          26.40           166.10   \n",
       "565         0.1752  ...        23.690          38.25           155.00   \n",
       "566         0.1590  ...        18.980          34.12           126.70   \n",
       "567         0.2397  ...        25.740          39.42           184.60   \n",
       "568         0.1587  ...         9.456          30.37            59.16   \n",
       "\n",
       "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0        2019.0           0.16220            0.66560           0.7119   \n",
       "1        1956.0           0.12380            0.18660           0.2416   \n",
       "2        1709.0           0.14440            0.42450           0.4504   \n",
       "3         567.7           0.20980            0.86630           0.6869   \n",
       "4        1575.0           0.13740            0.20500           0.4000   \n",
       "..          ...               ...                ...              ...   \n",
       "564      2027.0           0.14100            0.21130           0.4107   \n",
       "565      1731.0           0.11660            0.19220           0.3215   \n",
       "566      1124.0           0.11390            0.30940           0.3403   \n",
       "567      1821.0           0.16500            0.86810           0.9387   \n",
       "568       268.6           0.08996            0.06444           0.0000   \n",
       "\n",
       "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                  0.2654          0.4601                  0.11890  \n",
       "1                  0.1860          0.2750                  0.08902  \n",
       "2                  0.2430          0.3613                  0.08758  \n",
       "3                  0.2575          0.6638                  0.17300  \n",
       "4                  0.1625          0.2364                  0.07678  \n",
       "..                    ...             ...                      ...  \n",
       "564                0.2216          0.2060                  0.07115  \n",
       "565                0.1628          0.2572                  0.06637  \n",
       "566                0.1418          0.2218                  0.07820  \n",
       "567                0.2650          0.4087                  0.12400  \n",
       "568                0.0000          0.2871                  0.07039  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Some columns may not be useful for the model (For example, the first column contains ID number which may be irrelavant). \n",
    "# You need to get rid of the ID number feature.\n",
    "# Also you should transform target labels in the second column from 'B' and 'M' to 1 and -1.\n",
    "\n",
    "data = data.drop(columns=['id','Unnamed: 32'])\n",
    "def modDiagnosis(x):\n",
    "    if x=='B':\n",
    "        return 1\n",
    "    elif x=='M':\n",
    "        return -1\n",
    "    else:\n",
    "        return x\n",
    "data['diagnosis'] = data['diagnosis'].apply(modDiagnosis)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUD-NEwctRUH"
   },
   "source": [
    "## 1.3. Partition to training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "udlCdlgQtRUH",
    "outputId": "ce4032fe-e6e1-4170-fe3b-c7c8f349f3c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Shape:  (569, 31)\n",
      "Input Training Shape:  (455, 30)\n",
      "Output Training Shape:  (455,)\n",
      "Input Testing Shape:  (114, 30)\n",
      "Output Testing Shape:  (114,)\n"
     ]
    }
   ],
   "source": [
    "# You can partition using 80% training data and 20% testing data. It is a commonly used ratio in machinel learning.\n",
    "y = data['diagnosis'].to_numpy()\n",
    "x = data.drop(columns='diagnosis').to_numpy()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "\n",
    "print(\"Original Shape: \",data.shape)\n",
    "print(\"Input Training Shape: \",x_train.shape)\n",
    "print(\"Output Training Shape: \",y_train.shape)\n",
    "print(\"Input Testing Shape: \",x_test.shape)\n",
    "print(\"Output Testing Shape: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_uflmxYtRUI"
   },
   "source": [
    "## 1.4. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoF5Jjt5tRUI"
   },
   "source": [
    "Use the standardization to trainsform both training and test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VrI8gefjtRUI",
    "outputId": "a1ab2e88-8c50-4687-e764-c1244881928b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mean = \n",
      "[-0.05536291  0.00136489 -0.05431514 -0.0570866  -0.11460356 -0.02769166\n",
      " -0.05251674 -0.06917466 -0.01998588 -0.05334019 -0.03955586  0.12636415\n",
      " -0.02188444 -0.0463274   0.04603825  0.04304525 -0.04257918 -0.02085668\n",
      "  0.20802483 -0.03005862 -0.06814607  0.01641286 -0.06121343 -0.05884784\n",
      " -0.07662369 -0.04906964 -0.08229553 -0.10511583 -0.00620975 -0.10774898]\n",
      "test std = \n",
      "[0.95522453 0.94912308 0.95804691 0.93134423 1.08775129 1.12589852\n",
      " 1.0120803  0.99315792 1.2081299  1.01875435 0.82016937 0.99410342\n",
      " 0.83404494 0.75853384 0.97041774 1.237121   0.81949489 0.98020379\n",
      " 1.42342104 0.79542152 0.98191727 0.97881052 0.98770396 0.99728879\n",
      " 1.08905989 1.18082636 1.11500518 1.01261699 1.24424899 1.05391304]\n"
     ]
    }
   ],
   "source": [
    "# Standardization\n",
    "\n",
    "# calculate mu and sig using the training set\n",
    "d = x_train.shape[1]\n",
    "mu = numpy.mean(x_train, axis=0).reshape(1, d)\n",
    "sig = numpy.std(x_train, axis=0).reshape(1, d)\n",
    "\n",
    "# transform the training features\n",
    "x_train = (x_train - mu) / (sig + 1E-6)\n",
    "\n",
    "# transform the test features\n",
    "x_test = (x_test - mu) / (sig + 1E-6)\n",
    "\n",
    "print('test mean = ')\n",
    "print(numpy.mean(x_test, axis=0))\n",
    "\n",
    "print('test std = ')\n",
    "print(numpy.std(x_test, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58pUiIGBtRUJ"
   },
   "source": [
    "# 2.  Logistic Regression Model\n",
    "\n",
    "The objective function is $Q (w; X, y) = \\frac{1}{n} \\sum_{i=1}^n \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $.\n",
    "\n",
    "When $\\lambda = 0$, the model is a regular logistric regression and when $\\lambda > 0$, it essentially becomes a regularized logistric regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "tgJUJeK5tRUJ"
   },
   "outputs": [],
   "source": [
    "# Calculate the objective function value, or loss\n",
    "# Inputs:\n",
    "#     w: weight: d-by-1 matrix\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: regularization parameter: scalar\n",
    "# Return:\n",
    "#     objective function value, or loss (scalar)\n",
    "\n",
    "def objective(w, x, y, lam):\n",
    "    n,d = x.shape\n",
    "    sum = 0\n",
    "    for i in range(n):\n",
    "        innerproduct = numpy.dot(y[i]*numpy.transpose(x[i]),w)\n",
    "        logpart = numpy.log(1+numpy.exp(-innerproduct))\n",
    "        normpart = (lam/2)*(linalg.norm(w)**2)\n",
    "        sum += logpart+normpart\n",
    "    return (1/n)*sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1C7RDreRtRUJ"
   },
   "source": [
    "# 3. Numerical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M78CtbBItRUK"
   },
   "source": [
    "## 3.1. Gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bY5qioJ6tRUK"
   },
   "source": [
    "The gradient at $w$ for regularized logistic regression is  $g = - \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "T-eDU5aRtRUL"
   },
   "outputs": [],
   "source": [
    "# Calculate the gradient\n",
    "# Inputs:\n",
    "#     w: weight: d-by-1 matrix\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: regularization parameter: scalar\n",
    "# Return:\n",
    "#     g: gradient: d-by-1 matrix\n",
    "\n",
    "def gradient(w, x, y, lam):\n",
    "    n,d = x.shape  \n",
    "    sum = 0\n",
    "    for i in range(n):\n",
    "        numerator = y[i]*x[i]\n",
    "        innerproduct = numpy.dot(y[i]*numpy.transpose(x[i]),w)\n",
    "        denominator = 1+numpy.exp(innerproduct)\n",
    "        sum += (numerator/denominator) + (lam*w)\n",
    "    return (-1/n)*sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Z10wafF7tRUL"
   },
   "outputs": [],
   "source": [
    "# Gradient descent for solving logistic regression\n",
    "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
    "\n",
    "# Inputs:\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     learning_rate: scalar\n",
    "#     w: weights: d-by-1 matrix, initialization of w\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     w: weights: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "\n",
    "def gradient_descent(x, y, lam, learning_rate, w, max_epoch=100):\n",
    "    n,d = x.shape\n",
    "    objvals = numpy.zeros(max_epoch) \n",
    "    if w is None:\n",
    "        w = numpy.zeros(d) \n",
    "        \n",
    "    for t in range(max_epoch):\n",
    "        objval = objective(w, x, y, lam)\n",
    "        objvals[t] = objval\n",
    "        print(str(t)+\": \"+str(objval))\n",
    "        g = gradient(w, x, y, lam)\n",
    "        w -= (learning_rate*g)\n",
    "        \n",
    "    return w, objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cieB2Ip8tRUM"
   },
   "source": [
    "Use gradient_descent function to obtain your optimal weights and a list of objective values over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MqDJU7fxtRUM",
    "outputId": "46245a3a-9e14-4e65-f8c2-01f45a5b5a3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.6931471805599468\n",
      "1: 0.1816560964371292\n",
      "2: 0.1427237517522359\n",
      "3: 0.1275113553874095\n",
      "4: 0.12057761415454384\n",
      "5: 0.11574985240002088\n",
      "6: 0.11186538781499301\n",
      "7: 0.10858624137343656\n",
      "8: 0.10575184385705245\n",
      "9: 0.10326415424404205\n",
      "10: 0.10105555461097852\n",
      "11: 0.09907637840848409\n",
      "12: 0.09728877491191042\n",
      "13: 0.09566317830856286\n",
      "14: 0.09417606575886162\n",
      "15: 0.09280844325082915\n",
      "16: 0.09154478055189837\n",
      "17: 0.09037224022911679\n",
      "18: 0.08928010726486248\n",
      "19: 0.08825935962950546\n",
      "20: 0.08730234018654774\n",
      "21: 0.08640250279734847\n",
      "22: 0.08555421360477394\n",
      "23: 0.0847525939050903\n",
      "24: 0.08399339473987584\n",
      "25: 0.08327289594203709\n",
      "26: 0.08258782421959178\n",
      "27: 0.08193528619446522\n",
      "28: 0.08131271328742054\n",
      "29: 0.08071781605960698\n",
      "30: 0.08014854615818731\n",
      "31: 0.07960306441817203\n",
      "32: 0.07907971398031635\n",
      "33: 0.07857699752085626\n",
      "34: 0.07809355787117694\n",
      "35: 0.07762816144740917\n",
      "36: 0.07717968402117086\n",
      "37: 0.07674709845041225\n",
      "38: 0.07632946405897192\n",
      "39: 0.07592591740906009\n",
      "40: 0.07553566425553557\n",
      "41: 0.07515797250688847\n",
      "42: 0.07479216604707695\n",
      "43: 0.07443761929621198\n",
      "44: 0.07409375240760525\n",
      "45: 0.07376002701476118\n",
      "46: 0.07343594245516871\n",
      "47: 0.07312103240875462\n",
      "48: 0.07281486189803615\n",
      "49: 0.07251702460467452\n",
      "50: 0.07222714046356597\n",
      "51: 0.0719448535010226\n",
      "52: 0.07166982988817303\n",
      "53: 0.07140175618459191\n",
      "54: 0.07114033775046595\n",
      "55: 0.07088529730841608\n",
      "56: 0.07063637363850203\n",
      "57: 0.07039332039200255\n",
      "58: 0.07015590501133404\n",
      "59: 0.06992390774500545\n",
      "60: 0.06969712074783126\n",
      "61: 0.06947534725776769\n",
      "62: 0.06925840084174018\n",
      "63: 0.06904610470369005\n",
      "64: 0.06883829104883459\n",
      "65: 0.06863480049879056\n",
      "66: 0.0684354815527962\n",
      "67: 0.06824019009077305\n",
      "68: 0.06804878891442333\n",
      "69: 0.06786114732294872\n",
      "70: 0.06767714072032947\n",
      "71: 0.06749665025141259\n",
      "72: 0.06731956246432885\n",
      "73: 0.06714576899700761\n",
      "74: 0.06697516628576919\n",
      "75: 0.06680765529417286\n",
      "76: 0.06664314126046739\n",
      "77: 0.06648153346214727\n",
      "78: 0.06632274499625515\n",
      "79: 0.06616669257419396\n",
      "80: 0.06601329632992456\n",
      "81: 0.0658624796405241\n",
      "82: 0.06571416895816867\n",
      "83: 0.06556829365268803\n",
      "84: 0.06542478586390979\n",
      "85: 0.06528358036308009\n",
      "86: 0.06514461442270278\n",
      "87: 0.06500782769420005\n",
      "88: 0.06487316209283933\n",
      "89: 0.06474056168942156\n",
      "90: 0.06460997260826262\n",
      "91: 0.06448134293104031\n",
      "92: 0.06435462260610901\n",
      "93: 0.06422976336291636\n",
      "94: 0.06410671863118755\n",
      "95: 0.06398544346456154\n",
      "96: 0.06386589446839348\n",
      "97: 0.06374802973145513\n",
      "98: 0.06363180876128682\n",
      "99: 0.06351719242297008\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "w_gd,objvals_gd = gradient_descent(x_train,y_train,0,1,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bnUaAWqEtRUM",
    "outputId": "7776de0f-0369-4073-ac96-590b4c0ee7d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.6931471805599468\n",
      "1: 0.18261968487841387\n",
      "2: 0.14370009197583267\n",
      "3: 0.12848251905880229\n",
      "4: 0.12157098261794819\n",
      "5: 0.11678155281072468\n",
      "6: 0.11294242998726849\n",
      "7: 0.1097121073285844\n",
      "8: 0.10692829792677096\n",
      "9: 0.10449203214588534\n",
      "10: 0.10233515873830092\n",
      "11: 0.10040769185232555\n",
      "12: 0.09867158409487768\n",
      "13: 0.0970971465759278\n",
      "14: 0.09566077928153927\n",
      "15: 0.0943434405941457\n",
      "16: 0.09312957222754281\n",
      "17: 0.09200632189094597\n",
      "18: 0.09096296874902798\n",
      "19: 0.08999049120629116\n",
      "20: 0.08908123689947989\n",
      "21: 0.08822866746552242\n",
      "22: 0.08742715887647738\n",
      "23: 0.08667184362985211\n",
      "24: 0.08595848484630224\n",
      "25: 0.08528337495501484\n",
      "26: 0.08464325351344618\n",
      "27: 0.0840352400527589\n",
      "28: 0.08345677882163607\n",
      "29: 0.0829055930256538\n",
      "30: 0.08237964669992733\n",
      "31: 0.08187711275997041\n",
      "32: 0.0813963460852374\n",
      "33: 0.08093586072708331\n",
      "34: 0.0804943105161613\n",
      "35: 0.08007047248691168\n",
      "36: 0.07966323264856558\n",
      "37: 0.07927157372023888\n",
      "38: 0.07889456451766207\n",
      "39: 0.07853135073493656\n",
      "40: 0.07818114690955262\n",
      "41: 0.07784322939508817\n",
      "42: 0.07751693019536232\n",
      "43: 0.07720163153774656\n",
      "44: 0.07689676108292928\n",
      "45: 0.07660178768455105\n",
      "46: 0.07631621762543683\n",
      "47: 0.07603959126820159\n",
      "48: 0.0757714800671996\n",
      "49: 0.07551148389647361\n",
      "50: 0.07525922865481319\n",
      "51: 0.07501436411445593\n",
      "52: 0.0747765619845569\n",
      "53: 0.07454551416443116\n",
      "54: 0.07432093116488535\n",
      "55: 0.07410254067876537\n",
      "56: 0.07389008628426069\n",
      "57: 0.07368332626656887\n",
      "58: 0.0734820325453024\n",
      "59: 0.07328598969655069\n",
      "60: 0.07309499405983506\n",
      "61: 0.07290885292134093\n",
      "62: 0.07272738376580883\n",
      "63: 0.07255041359033373\n",
      "64: 0.072377778274078\n",
      "65: 0.07220932199856692\n",
      "66: 0.07204489671381573\n",
      "67: 0.0718843616460457\n",
      "68: 0.07172758284319494\n",
      "69: 0.07157443275482782\n",
      "70: 0.07142478984339141\n",
      "71: 0.07127853822407935\n",
      "72: 0.07113556733083731\n",
      "73: 0.07099577160628523\n",
      "74: 0.07085905021355018\n",
      "75: 0.07072530676819425\n",
      "76: 0.0705944490885958\n",
      "77: 0.07046638896329392\n",
      "78: 0.07034104193394486\n",
      "79: 0.0702183270926625\n",
      "80: 0.07009816689262434\n",
      "81: 0.06998048697092427\n",
      "82: 0.06986521598274532\n",
      "83: 0.06975228544600086\n",
      "84: 0.06964162959567205\n",
      "85: 0.06953318524713002\n",
      "86: 0.06942689166779165\n",
      "87: 0.0693226904565163\n",
      "88: 0.06922052543019157\n",
      "89: 0.06912034251700808\n",
      "90: 0.06902208965595898\n",
      "91: 0.06892571670213955\n",
      "92: 0.06883117533745194\n",
      "93: 0.06873841898635515\n",
      "94: 0.06864740273632466\n",
      "95: 0.0685580832627128\n",
      "96: 0.0684704187577238\n",
      "97: 0.06838436886324092\n",
      "98: 0.06829989460725722\n",
      "99: 0.0682169583436857\n"
     ]
    }
   ],
   "source": [
    "# Train regularized logistric regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "w_gd_reg,objvals_gd_reg = gradient_descent(x_train,y_train,1E-3,1,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVHO8z2ctRUN"
   },
   "source": [
    "## 3.2. Stochastic gradient descent (SGD)\n",
    "\n",
    "Define new objective function $Q_i (w) = \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $. \n",
    "\n",
    "The stochastic gradient at $w$ is $g_i = \\frac{\\partial Q_i }{ \\partial w} = -\\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
    "\n",
    "You may need to implement a new function to calculate the new objective function and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "JJaRknUOtRUN"
   },
   "outputs": [],
   "source": [
    "# Calculate the objective Q_i and the gradient of Q_i\n",
    "# Inputs:\n",
    "#     w: weights: d-by-1 matrix\n",
    "#     xi: data: 1-by-d matrix\n",
    "#     yi: label: scalar\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "\n",
    "def stochastic_objective_gradient(w, xi, yi, lam):\n",
    "    numerator = -yi*xi\n",
    "    innerproduct = numpy.dot(yi*numpy.transpose(xi),w)\n",
    "    denominator = 1+numpy.exp(innerproduct)\n",
    "    logpart = numpy.log(1+numpy.exp(-innerproduct))\n",
    "    normpart = (lam/2)*(linalg.norm(w)**2)\n",
    "    g = (numerator/denominator) + (lam*w)\n",
    "    obj = logpart+normpart\n",
    "    return obj,g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NcKPYYhtRUN"
   },
   "source": [
    "Hints:\n",
    "1. In every epoch, randomly permute the $n$ samples.\n",
    "2. Each epoch has $n$ iterations. In every iteration, use 1 sample, and compute the gradient and objective using the ``stochastic_objective_gradient`` function. In the next iteration, use the next sample, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "hu333q1UtRUO"
   },
   "outputs": [],
   "source": [
    "# SGD for solving logistic regression\n",
    "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
    "\n",
    "# Inputs:\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     learning_rate: scalar\n",
    "#     w: weights: d-by-1 matrix, initialization of w\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     \n",
    "#     w: weights: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "#     Record one objective value per epoch (not per iteration)\n",
    "\n",
    "def sgd(x, y, lam, learning_rate, w, max_epoch=100):\n",
    "    n,d = x.shape\n",
    "    objvals = numpy.zeros(max_epoch)\n",
    "    if w is None:\n",
    "        w = numpy.zeros(d)\n",
    "\n",
    "    for t in range(max_epoch):\n",
    "        rand_indices = numpy.random.permutation(n)\n",
    "        x_rand = x[rand_indices]\n",
    "        y_rand = y[rand_indices]\n",
    "        objval = 0\n",
    "\n",
    "        for i in range(n):\n",
    "            xi = x_rand[i]\n",
    "            yi = y_rand[i]\n",
    "            obj,g = stochastic_objective_gradient(w, xi, yi, lam)\n",
    "            objval += obj\n",
    "            # objval = obj\n",
    "            w -= (learning_rate*g)\n",
    "            # for j in range(d-1):\n",
    "            #     w[j+1] = w[j]-(learning_rate*g[j])\n",
    "\n",
    "        objval /= n\n",
    "        objvals[t] = objval\n",
    "        print(str(t)+\": \"+str(objval))\n",
    "        # learning_rate*=.9\n",
    "\n",
    "    return w, objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlmUj1DQtRUO"
   },
   "source": [
    "Use sgd function to obtain your optimal weights and a list of objective values over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6CcLnZMtRUO",
    "outputId": "201a1d05-4b5c-4d01-f77c-49b0b3438551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.27231714647546434\n",
      "1: 0.44201847596317\n",
      "2: 0.39744064464717244\n",
      "3: 0.45424781997283376\n",
      "4: 0.33706405628866415\n",
      "5: 0.31452611905595723\n",
      "6: 0.22809881942380505\n",
      "7: 0.35946647937649645\n",
      "8: 0.29393247726097804\n",
      "9: 0.2574702415876981\n",
      "10: 0.3039458666724603\n",
      "11: 0.24344259255740514\n",
      "12: 0.21122437523826057\n",
      "13: 0.3390846949155613\n",
      "14: 0.2506224960196384\n",
      "15: 0.18582202697568637\n",
      "16: 0.17587692087487253\n",
      "17: 0.1619474923377069\n",
      "18: 0.194584114646831\n",
      "19: 0.2553674059517392\n",
      "20: 0.353923776643572\n",
      "21: 0.2003286710204954\n",
      "22: 0.1958184087858527\n",
      "23: 0.16946649491326862\n",
      "24: 0.2685791145071976\n",
      "25: 0.2028526507124045\n",
      "26: 0.32230251136155236\n",
      "27: 0.32756847781659937\n",
      "28: 0.21653032000768668\n",
      "29: 0.19532916225466587\n",
      "30: 0.20510824387798468\n",
      "31: 0.19792765476856033\n",
      "32: 0.1773483665040033\n",
      "33: 0.19554449095055024\n",
      "34: 0.32115149405182236\n",
      "35: 0.3364041036966144\n",
      "36: 0.20071819610154407\n",
      "37: 0.1937025485886955\n",
      "38: 0.24744780660932747\n",
      "39: 0.21029270967958305\n",
      "40: 0.20000651936459438\n",
      "41: 0.3161270942576889\n",
      "42: 0.1709454223296258\n",
      "43: 0.19931083489836562\n",
      "44: 0.13226877366111925\n",
      "45: 0.1479455878211362\n",
      "46: 0.1773733272443616\n",
      "47: 0.23176794970992262\n",
      "48: 0.1592909530144744\n",
      "49: 0.2131043331149711\n",
      "50: 0.16322301574191006\n",
      "51: 0.18422423467154891\n",
      "52: 0.12290045811381803\n",
      "53: 0.17060195637640121\n",
      "54: 0.1531706553364587\n",
      "55: 0.14196532422635808\n",
      "56: 0.3317930771176411\n",
      "57: 0.19318287552033736\n",
      "58: 0.1914959832696738\n",
      "59: 0.218973785822253\n",
      "60: 0.1797972011617692\n",
      "61: 0.37097690715060555\n",
      "62: 0.17011931337684993\n",
      "63: 0.17130887245403165\n",
      "64: 0.14583573246411174\n",
      "65: 0.16446100416675818\n",
      "66: 0.1428408789746162\n",
      "67: 0.2462022269269074\n",
      "68: 0.2809663476869119\n",
      "69: 0.1870362481220651\n",
      "70: 0.228210340734928\n",
      "71: 0.1933014333887017\n",
      "72: 0.13749647146712043\n",
      "73: 0.17049018830320256\n",
      "74: 0.15906880517470823\n",
      "75: 0.1491151172290657\n",
      "76: 0.185260355686622\n",
      "77: 0.11555603817791621\n",
      "78: 0.1948696033180781\n",
      "79: 0.24114036495900185\n",
      "80: 0.19720062415230522\n",
      "81: 0.16796287189898243\n",
      "82: 0.132550354557079\n",
      "83: 0.1298202375898782\n",
      "84: 0.1313828213189405\n",
      "85: 0.15065027601928943\n",
      "86: 0.11875530682475648\n",
      "87: 0.11858897778795778\n",
      "88: 0.11513486706756809\n",
      "89: 0.13843476809617725\n",
      "90: 0.12823725138742434\n",
      "91: 0.3556997806234853\n",
      "92: 0.18336306847278044\n",
      "93: 0.11628522514238977\n",
      "94: 0.14365015563608904\n",
      "95: 0.18237610199086846\n",
      "96: 0.2411820554853962\n",
      "97: 0.19200763429281617\n",
      "98: 0.143336329233252\n",
      "99: 0.13347040129575577\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "w_sgd,objvals_sgd = sgd(x_train,y_train,0,1,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYq2gnOAtRUO",
    "outputId": "b30a758b-3ac4-4cd9-8f4d-4095be28c26e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.6838765680476026\n",
      "1: 0.619966844378524\n",
      "2: 0.36886666571704685\n",
      "3: 0.4669308723760206\n",
      "4: 0.6147304883629386\n",
      "5: 0.4579930231099857\n",
      "6: 0.432228649308341\n",
      "7: 0.5106928530691915\n",
      "8: 0.6366372836821541\n",
      "9: 0.36209776337580424\n",
      "10: 0.1774186976310755\n",
      "11: 0.3812213281951602\n",
      "12: 0.3294631625980738\n",
      "13: 0.23191441284261444\n",
      "14: 0.6804547340163127\n",
      "15: 0.45887904587872264\n",
      "16: 0.6604771111239678\n",
      "17: 0.3858275168884064\n",
      "18: 0.2993956192337001\n",
      "19: 0.29837826174482734\n",
      "20: 0.5754749400465724\n",
      "21: 0.4454452073654004\n",
      "22: 0.42711617881073977\n",
      "23: 0.6069441355612153\n",
      "24: 0.47654063659951856\n",
      "25: 0.4889302990309202\n",
      "26: 0.3307523554306926\n",
      "27: 0.4902662324873171\n",
      "28: 0.2560677449910021\n",
      "29: 0.4667886670179426\n",
      "30: 0.32688319604494553\n",
      "31: 0.6487476877452025\n",
      "32: 0.723525125408606\n",
      "33: 0.4896444077223802\n",
      "34: 0.45121037620364896\n",
      "35: 0.42348426320621185\n",
      "36: 0.5634171412644211\n",
      "37: 0.5784710577600113\n",
      "38: 0.4539794310295924\n",
      "39: 0.4749633273420008\n",
      "40: 0.33237914341328073\n",
      "41: 0.6561435049392829\n",
      "42: 0.4176624288578728\n",
      "43: 0.6510276039794505\n",
      "44: 0.5325881210537544\n",
      "45: 0.5707611605986057\n",
      "46: 0.4943367148580726\n",
      "47: 0.3450180885996006\n",
      "48: 0.6743313633449932\n",
      "49: 0.41967460848275345\n",
      "50: 0.28126446079322265\n",
      "51: 0.5931657303660683\n",
      "52: 0.44517115756992237\n",
      "53: 0.43834434757966917\n",
      "54: 0.4209461538119906\n",
      "55: 0.3595217668480261\n",
      "56: 0.5760215391754923\n",
      "57: 0.4750174927909113\n",
      "58: 0.1714931924632742\n",
      "59: 0.3759658103317572\n",
      "60: 0.5805518376410205\n",
      "61: 0.35022359633537803\n",
      "62: 0.40585035072964815\n",
      "63: 0.3130315991168177\n",
      "64: 0.3442854267538953\n",
      "65: 0.5917463625631505\n",
      "66: 0.4638752800465831\n",
      "67: 0.31528415741558674\n",
      "68: 0.5855413677974677\n",
      "69: 0.5353489202322398\n",
      "70: 0.34302037824323456\n",
      "71: 0.34814393797331156\n",
      "72: 0.6977813971707518\n",
      "73: 0.374871249794518\n",
      "74: 0.3428632400189096\n",
      "75: 0.27027061988257867\n",
      "76: 0.4930708656698871\n",
      "77: 0.5814813901384557\n",
      "78: 0.32302908242554046\n",
      "79: 0.5268487404224697\n",
      "80: 0.47177147977879236\n",
      "81: 0.596509277996288\n",
      "82: 0.6129611530286403\n",
      "83: 0.46720418066946157\n",
      "84: 0.5360636751864207\n",
      "85: 0.40328413371974137\n",
      "86: 0.5784168560188118\n",
      "87: 0.6491893691842066\n",
      "88: 0.44370701012171154\n",
      "89: 0.2334776479587222\n",
      "90: 0.6737208964079714\n",
      "91: 0.5582196107696119\n",
      "92: 0.33013832644473784\n",
      "93: 0.1783364435473781\n",
      "94: 0.20624145762607202\n",
      "95: 0.3692325042634405\n",
      "96: 0.574904088033455\n",
      "97: 0.35884609039424376\n",
      "98: 0.6957498035218393\n",
      "99: 0.5792265655523015\n"
     ]
    }
   ],
   "source": [
    "# Train regularized logistric regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "w_sgd_reg,objvals_sgd_reg = sgd(x_train,y_train,1E-3,1,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30vVm_8etRUO"
   },
   "source": [
    "## 3.3 Mini-Batch Gradient Descent (MBGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6kUlh5ftRUO"
   },
   "source": [
    "Define $Q_I (w) = \\frac{1}{b} \\sum_{i \\in I} \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $, where $I$ is a set containing $b$ indices randomly drawn from $\\{ 1, \\cdots , n \\}$ without replacement.\n",
    "\n",
    "The stochastic gradient at $w$ is $g_I = \\frac{\\partial Q_I }{ \\partial w} = \\frac{1}{b} \\sum_{i \\in I} \\frac{- y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
    "\n",
    "You may need to implement a new function to calculate the new objective function and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "V76EX7gKtRUO"
   },
   "outputs": [],
   "source": [
    "# Calculate the objective Q_I and the gradient of Q_I\n",
    "# Inputs:\n",
    "#     w: weights: d-by-1 matrix\n",
    "#     xi: data: b-by-d matrix\n",
    "#     yi: label: b-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "\n",
    "def mb_objective_gradient(w, xi, yi, lam):\n",
    "    b,d = xi.shape\n",
    "    sum_obj = 0\n",
    "    sum_g = 0\n",
    "    for i in range(b):\n",
    "        numerator = -yi[i]*xi[i]\n",
    "        innerproduct = numpy.dot(yi[i]*numpy.transpose(xi[i]),w)\n",
    "        denominator = 1+numpy.exp(innerproduct)\n",
    "        logpart = numpy.log(1+numpy.exp(-innerproduct))\n",
    "        normpart = (lam/2)*(linalg.norm(w)**2)\n",
    "        sum_obj += logpart+normpart\n",
    "        sum_g += ((numerator/denominator)+(lam*w))\n",
    "    obj = sum_obj/b\n",
    "    g = sum_g/b\n",
    "    return obj,g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Mv82EuctRUP"
   },
   "source": [
    "Hints:\n",
    "1. In every epoch, randomly permute the $n$ samples (just like SGD).\n",
    "2. Each epoch has $\\frac{n}{b}$ iterations. In every iteration, use $b$ samples, and compute the gradient and objective using the ``mb_objective_gradient`` function. In the next iteration, use the next $b$ samples, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "JcrenghjtRUP"
   },
   "outputs": [],
   "source": [
    "# MBGD for solving logistic regression\n",
    "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
    "\n",
    "# Inputs:\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     learning_rate: scalar\n",
    "#     b: batch_size: scalar\n",
    "#     w: weights: d-by-1 matrix, initialization of w\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     w: weights: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "#     Record one objective value per epoch (not per iteration)\n",
    "\n",
    "def mbgd(x, y, lam, learning_rate, b, w, max_epoch=100):\n",
    "    n,d = x.shape\n",
    "    objvals = numpy.zeros(max_epoch)\n",
    "    if w is None:\n",
    "        w = numpy.zeros(d)\n",
    "    \n",
    "    for t in range(max_epoch):\n",
    "        rand_indices = numpy.random.permutation(n)\n",
    "        x_rand = x[rand_indices]\n",
    "        y_rand = y[rand_indices]\n",
    "        objval = 0\n",
    "        count = 0\n",
    "        i = 0\n",
    "\n",
    "        while i<n:\n",
    "            xi = []\n",
    "            yi = []\n",
    "            if (i+b)>n:\n",
    "                xi = x_rand[i:]\n",
    "                yi = y_rand[i:]\n",
    "                i=n\n",
    "            else:\n",
    "                xi = x_rand[i:(i+b)]\n",
    "                yi = y_rand[i:(i+b)]\n",
    "                i+=b\n",
    "            obj,g = mb_objective_gradient(w,xi,yi,lam)\n",
    "            objval += obj\n",
    "            # objval = obj\n",
    "            # for j in range(d-1):\n",
    "            #     w[j+1] = w[j]-(learning_rate*g[j])\n",
    "            w -= (learning_rate*g)\n",
    "            count+=1\n",
    "\n",
    "        objval/=count\n",
    "        objvals[t] = objval\n",
    "        print(str(t)+\": \"+str(objval))\n",
    "        # learning_rate*=.9\n",
    "      \n",
    "    return w,objvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exzAdUTltRUP"
   },
   "source": [
    "Use mbgd function to obtain your optimal weights and a list of objective values over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qMf6ZLSWtRUP",
    "outputId": "cd030782-3282-4370-c518-b52866f2d0e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.13653462540036954\n",
      "1: 0.10247041762905737\n",
      "2: 0.07805660663919743\n",
      "3: 0.09335354808122642\n",
      "4: 0.0815158981308097\n",
      "5: 0.06636308981146447\n",
      "6: 0.06643148116903602\n",
      "7: 0.08470525091369623\n",
      "8: 0.07476119087613725\n",
      "9: 0.0689807162950102\n",
      "10: 0.0629113446555109\n",
      "11: 0.07195311115961826\n",
      "12: 0.07752266437880051\n",
      "13: 0.06141948174930883\n",
      "14: 0.06004821880526556\n",
      "15: 0.05958383731028593\n",
      "16: 0.08349434225685394\n",
      "17: 0.0772063830590084\n",
      "18: 0.05673547050102102\n",
      "19: 0.057997349768000805\n",
      "20: 0.0776914927338067\n",
      "21: 0.05658375370952729\n",
      "22: 0.05643480089990006\n",
      "23: 0.06029044100257942\n",
      "24: 0.05480397883539817\n",
      "25: 0.05786662414256489\n",
      "26: 0.07822674402058806\n",
      "27: 0.06966583380867968\n",
      "28: 0.061716882157347136\n",
      "29: 0.05555506653486281\n",
      "30: 0.06044744220377146\n",
      "31: 0.06653187648672654\n",
      "32: 0.05832912458301698\n",
      "33: 0.05504944392243969\n",
      "34: 0.05549390463120071\n",
      "35: 0.05992139981579992\n",
      "36: 0.05518590521375047\n",
      "37: 0.05527455394225202\n",
      "38: 0.060707576372612836\n",
      "39: 0.055095512001335445\n",
      "40: 0.057921857742961914\n",
      "41: 0.05429251249489975\n",
      "42: 0.05469236739302548\n",
      "43: 0.061004111979710805\n",
      "44: 0.07583389504247454\n",
      "45: 0.06348453630585052\n",
      "46: 0.05397434895385521\n",
      "47: 0.05675559465920283\n",
      "48: 0.05357152858278351\n",
      "49: 0.053440070881461306\n",
      "50: 0.049978132246691336\n",
      "51: 0.05238054894553457\n",
      "52: 0.05244912380052961\n",
      "53: 0.051612865708405266\n",
      "54: 0.05868032189469981\n",
      "55: 0.05494743125258296\n",
      "56: 0.056808225507748195\n",
      "57: 0.05321674395631399\n",
      "58: 0.06048370310671602\n",
      "59: 0.05466723735477094\n",
      "60: 0.048056638373307294\n",
      "61: 0.053478160065102205\n",
      "62: 0.0557288657300127\n",
      "63: 0.050942117000270314\n",
      "64: 0.05892327297007906\n",
      "65: 0.062388481425849115\n",
      "66: 0.0638833844908744\n",
      "67: 0.05819978597418065\n",
      "68: 0.0539522267494188\n",
      "69: 0.05280503334213991\n",
      "70: 0.05981130144954103\n",
      "71: 0.05551285326454694\n",
      "72: 0.050527711315664754\n",
      "73: 0.05277742889124778\n",
      "74: 0.04970289474374917\n",
      "75: 0.055017632315097174\n",
      "76: 0.06415060116727572\n",
      "77: 0.05424165409824734\n",
      "78: 0.0498135287581592\n",
      "79: 0.05470014606204029\n",
      "80: 0.045140815320411934\n",
      "81: 0.06393535251926295\n",
      "82: 0.05222324308850332\n",
      "83: 0.05215344563878758\n",
      "84: 0.05104345465199151\n",
      "85: 0.047167875539998294\n",
      "86: 0.0475529560456863\n",
      "87: 0.04547150521183491\n",
      "88: 0.04955575409060059\n",
      "89: 0.05096143834857388\n",
      "90: 0.046501012127110226\n",
      "91: 0.06119604248030237\n",
      "92: 0.06444495570525809\n",
      "93: 0.05096395899119761\n",
      "94: 0.0498350544486049\n",
      "95: 0.05143678854309442\n",
      "96: 0.04794784903109094\n",
      "97: 0.057452683318488976\n",
      "98: 0.05048637147888964\n",
      "99: 0.047929932230390604\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "w_mbgd,objvals_mbgd = mbgd(x_train,y_train,0,1,5,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6YUiYpf0tRUP",
    "outputId": "e4ea08e3-cb4f-47db-9a0c-66f125062cf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.10638856086348403\n",
      "1: 0.09087478815156563\n",
      "2: 0.10113668048725567\n",
      "3: 0.11393403442953233\n",
      "4: 0.11344641895042878\n",
      "5: 0.11398683435690593\n",
      "6: 0.1029580518844983\n",
      "7: 0.09398923375459649\n",
      "8: 0.09023929237693926\n",
      "9: 0.08876917782116034\n",
      "10: 0.07995246726353773\n",
      "11: 0.11752023277589344\n",
      "12: 0.10579773278150359\n",
      "13: 0.10778516175196148\n",
      "14: 0.07930885261454054\n",
      "15: 0.0946550965731325\n",
      "16: 0.0845590834029573\n",
      "17: 0.08409153748050109\n",
      "18: 0.08186011176414944\n",
      "19: 0.08179338750369973\n",
      "20: 0.08266997666645715\n",
      "21: 0.10844510613516793\n",
      "22: 0.0858120942889271\n",
      "23: 0.08319346538910952\n",
      "24: 0.097730499071771\n",
      "25: 0.09619469865699502\n",
      "26: 0.0793378557191816\n",
      "27: 0.07595777537224006\n",
      "28: 0.0925294945961215\n",
      "29: 0.07841288443127037\n",
      "30: 0.0924193082341071\n",
      "31: 0.09188714765214913\n",
      "32: 0.07768297438447096\n",
      "33: 0.07525672291321445\n",
      "34: 0.09680541195964233\n",
      "35: 0.12492999987258883\n",
      "36: 0.10251433398423751\n",
      "37: 0.08258454995213318\n",
      "38: 0.12538263331194102\n",
      "39: 0.08481511010877121\n",
      "40: 0.09774591032698894\n",
      "41: 0.08575134082430501\n",
      "42: 0.11136185919992016\n",
      "43: 0.08017060205105954\n",
      "44: 0.09022212094742546\n",
      "45: 0.07915272548086794\n",
      "46: 0.1145593830238414\n",
      "47: 0.09567068662331958\n",
      "48: 0.08822459846073528\n",
      "49: 0.10030143737563273\n",
      "50: 0.0844494071582746\n",
      "51: 0.09652251091476291\n",
      "52: 0.09026129555005953\n",
      "53: 0.08954789451758191\n",
      "54: 0.08475083309901028\n",
      "55: 0.09336786625571947\n",
      "56: 0.11378897106416562\n",
      "57: 0.0884326049742008\n",
      "58: 0.08346643132830985\n",
      "59: 0.09481227795174074\n",
      "60: 0.11501352798382698\n",
      "61: 0.09113803031020558\n",
      "62: 0.10161272504705023\n",
      "63: 0.08408344347329176\n",
      "64: 0.07339225165444\n",
      "65: 0.08431680578333925\n",
      "66: 0.12878695586711708\n",
      "67: 0.10176922955181888\n",
      "68: 0.08427603045207765\n",
      "69: 0.09403292622981281\n",
      "70: 0.09753732033343657\n",
      "71: 0.09206144362450536\n",
      "72: 0.08219499738443517\n",
      "73: 0.08025114619857354\n",
      "74: 0.09328293516647002\n",
      "75: 0.08528364458182867\n",
      "76: 0.081267104209675\n",
      "77: 0.07734301074780635\n",
      "78: 0.09344121914150498\n",
      "79: 0.12347613098779378\n",
      "80: 0.11291760984145302\n",
      "81: 0.08131271021819135\n",
      "82: 0.07659259063169231\n",
      "83: 0.0758093446792118\n",
      "84: 0.09767168749784297\n",
      "85: 0.07982453471778146\n",
      "86: 0.07322205413986813\n",
      "87: 0.07253973159157333\n",
      "88: 0.08424513644349148\n",
      "89: 0.08588084576220095\n",
      "90: 0.07597868238896754\n",
      "91: 0.09814481857936229\n",
      "92: 0.07399134435076483\n",
      "93: 0.09904731447060493\n",
      "94: 0.10998762592094839\n",
      "95: 0.08449540460741717\n",
      "96: 0.07857130544477459\n",
      "97: 0.09923660002433331\n",
      "98: 0.07949942711214236\n",
      "99: 0.0738952289420098\n"
     ]
    }
   ],
   "source": [
    "# Train regularized logistric regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "w_mbgd_reg,objvals_mbgd_reg = mbgd(x_train,y_train,1E-3,1,5,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUnR54tPtRUP"
   },
   "source": [
    "# 4. Compare GD, SGD, MBGD\n",
    "\n",
    "### Plot objective function values against epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "3zQKyCYjtRUQ",
    "outputId": "872fd13d-7721-49bd-d99d-1339241c74fa",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "e_gd = range(len(objvals_gd))\n",
    "e_sgd = range(len(objvals_sgd))\n",
    "e_mbgd = range(len(objvals_mbgd))\n",
    "\n",
    "l_gd, = plt.plot(e_gd, objvals_gd, '-b', linewidth=2)\n",
    "l_sgd, = plt.plot(e_sgd, objvals_sgd, '-r', linewidth=2)\n",
    "l_mbgd, = plt.plot(e_mbgd, objvals_mbgd,'-g', linewidth=2)\n",
    "\n",
    "plt.xlabel('Epochs', fontsize=20)\n",
    "plt.ylabel('Objective Value', fontsize=20)\n",
    "\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.legend([l_gd, l_sgd, l_mbgd], ['GD', 'SGD','MBGD'], fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQmIiQwntRUQ"
   },
   "source": [
    "# 5. Prediction\n",
    "### Compare the training and testing accuracy for logistic regression and regularized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "eEyUxoI5tRUQ"
   },
   "outputs": [],
   "source": [
    "# Predict class label\n",
    "# Inputs:\n",
    "#     w: weights: d-by-1 matrix\n",
    "#     X: data: m-by-d matrix\n",
    "# Return:\n",
    "#     f: m-by-1 matrix, the predictions\n",
    "def predict(w, X):\n",
    "    f = numpy.sign(numpy.dot(X,w))\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y6asiZLptRUQ",
    "outputId": "436a29df-8c49-4d31-94e2-287d7037c691"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD Error: 0.015384615384615385\n",
      "GD Reg Error: 0.015384615384615385\n",
      "SGD Error: 0.03956043956043956\n",
      "SGD Reg Error: 0.026373626373626374\n",
      "MBGD Error: 0.01098901098901099\n",
      "MBGD Reg Error: 0.015384615384615385\n"
     ]
    }
   ],
   "source": [
    "# evaluate training error of logistric regression and regularized version\n",
    "f_gd = predict(w_gd,x_train) \n",
    "gd_error = numpy.mean(numpy.abs(f_gd-y_train)/2)\n",
    "print(\"GD Error: \"+str(gd_error))\n",
    "f_gd_reg = predict(w_gd_reg,x_train)\n",
    "gd_reg_error = numpy.mean(numpy.abs(f_gd_reg-y_train)/2)\n",
    "print(\"GD Reg Error: \"+str(gd_reg_error))\n",
    "f_sgd = predict(w_sgd,x_train) \n",
    "sgd_error = numpy.mean(numpy.abs(f_sgd-y_train)/2)\n",
    "print(\"SGD Error: \"+str(sgd_error))\n",
    "f_sgd_reg = predict(w_sgd_reg,x_train)\n",
    "sgd_reg_error = numpy.mean(numpy.abs(f_sgd_reg-y_train)/2)\n",
    "print(\"SGD Reg Error: \"+str(sgd_reg_error))\n",
    "f_mbgd = predict(w_mbgd,x_train) \n",
    "mbgd_error = numpy.mean(numpy.abs(f_mbgd-y_train)/2)\n",
    "print(\"MBGD Error: \"+str(mbgd_error))\n",
    "f_mbgd_reg = predict(w_mbgd_reg,x_train)\n",
    "mbgd_reg_error = numpy.mean(numpy.abs(f_mbgd_reg-y_train)/2)\n",
    "print(\"MBGD Reg Error: \"+str(mbgd_reg_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K3XFcXCotRUQ",
    "outputId": "662a3846-a9bc-43e8-fd25-0b2890ec8775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD Error: 0.017543859649122806\n",
      "GD Reg Error: 0.017543859649122806\n",
      "SGD Error: 0.08771929824561403\n",
      "SGD Reg Error: 0.02631578947368421\n",
      "MBGD Error: 0.03508771929824561\n",
      "MBGD Reg Error: 0.02631578947368421\n"
     ]
    }
   ],
   "source": [
    "# evaluate testing error of logistric regression and regularized version\n",
    "f_gd = predict(w_gd,x_test) \n",
    "gd_error = numpy.mean(numpy.abs(f_gd-y_test)/2)\n",
    "print(\"GD Error: \"+str(gd_error))\n",
    "f_gd_reg = predict(w_gd_reg,x_test)\n",
    "gd_reg_error = numpy.mean(numpy.abs(f_gd_reg-y_test)/2)\n",
    "print(\"GD Reg Error: \"+str(gd_reg_error))\n",
    "f_sgd = predict(w_sgd,x_test) \n",
    "sgd_error = numpy.mean(numpy.abs(f_sgd-y_test)/2)\n",
    "print(\"SGD Error: \"+str(sgd_error))\n",
    "f_sgd_reg = predict(w_sgd_reg,x_test)\n",
    "sgd_reg_error = numpy.mean(numpy.abs(f_sgd_reg-y_test)/2)\n",
    "print(\"SGD Reg Error: \"+str(sgd_reg_error))\n",
    "f_mbgd = predict(w_mbgd,x_test) \n",
    "mbgd_error = numpy.mean(numpy.abs(f_mbgd-y_test)/2)\n",
    "print(\"MBGD Error: \"+str(mbgd_error))\n",
    "f_mbgd_reg = predict(w_mbgd_reg,x_test)\n",
    "mbgd_reg_error = numpy.mean(numpy.abs(f_mbgd_reg-y_test)/2)\n",
    "print(\"MBGD Reg Error: \"+str(mbgd_reg_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Px2FDbsQtRUQ"
   },
   "source": [
    "# 6. Parameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iZQ7aWNtRUQ"
   },
   "source": [
    "### In this section, you may try different combinations of parameters (regularization value, learning rate, etc) to see their effects on the model. (Open ended question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "frtwxxLEtRUR",
    "outputId": "42f874b6-3181-4d6c-a8ef-8ad61f4a0a90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent:\n",
      "GD Low:\n",
      "0: 0.6931471805599468\n",
      "1: 0.24777612030245974\n",
      "2: 0.21221119185730614\n",
      "3: 0.1901794801970291\n",
      "4: 0.1750004922757288\n",
      "5: 0.16377277554160644\n",
      "6: 0.15503261007326238\n",
      "7: 0.14797256240819442\n",
      "8: 0.14211105215120584\n",
      "9: 0.1371410532753878\n",
      "10: 0.1328560937882527\n",
      "11: 0.12911132711109483\n",
      "12: 0.1258016277941196\n",
      "13: 0.12284854452656091\n",
      "14: 0.120192154271497\n",
      "15: 0.11778577365928182\n",
      "16: 0.11559241068652645\n",
      "17: 0.11358231637292943\n",
      "18: 0.11173125398953639\n",
      "19: 0.11001924938913465\n",
      "20: 0.10842967170750233\n",
      "21: 0.10694854577404803\n",
      "22: 0.10556403012186295\n",
      "23: 0.1042660153644021\n",
      "24: 0.10304581140478741\n",
      "25: 0.10189590111842277\n",
      "26: 0.10080974440915566\n",
      "27: 0.09978162088239319\n",
      "28: 0.09880650243879101\n",
      "29: 0.09787994927901421\n",
      "30: 0.09699802439332891\n",
      "31: 0.09615722276988574\n",
      "32: 0.09535441241515993\n",
      "33: 0.09458678492357923\n",
      "34: 0.0938518138199094\n",
      "35: 0.09314721926911469\n",
      "36: 0.09247093803397036\n",
      "37: 0.09182109778215464\n",
      "38: 0.09119599501758474\n",
      "39: 0.0905940760469276\n",
      "40: 0.0900139205000786\n",
      "41: 0.08945422700939776\n",
      "42: 0.08891380072145375\n",
      "43: 0.08839154237064906\n",
      "44: 0.08788643868920441\n",
      "45: 0.0873975539647437\n",
      "46: 0.08692402258683059\n",
      "47: 0.08646504244858248\n",
      "48: 0.0860198690899725\n",
      "49: 0.08558781048641807\n",
      "50: 0.0851682224004282\n",
      "51: 0.08476050422592721\n",
      "52: 0.08436409526482647\n",
      "53: 0.08397847138380184\n",
      "54: 0.08360314200632381\n",
      "55: 0.08323764740100487\n",
      "56: 0.08288155623244886\n",
      "57: 0.08253446334515398\n",
      "58: 0.08219598775476633\n",
      "59: 0.08186577082418989\n",
      "60: 0.08154347460482439\n",
      "61: 0.0812287803255894\n",
      "62: 0.08092138701445496\n",
      "63: 0.0806210102389942\n",
      "64: 0.08032738095402459\n",
      "65: 0.08004024444576532\n",
      "66: 0.07975935936311976\n",
      "67: 0.07948449682773048\n",
      "68: 0.07921543961536179\n",
      "69: 0.07895198140196315\n",
      "70: 0.07869392606847271\n",
      "71: 0.07844108705903428\n",
      "72: 0.07819328678785645\n",
      "73: 0.07795035609042049\n",
      "74: 0.07771213371517888\n",
      "75: 0.07747846585226409\n",
      "76: 0.0772492056960703\n",
      "77: 0.0770242130388705\n",
      "78: 0.07680335389290427\n",
      "79: 0.07658650013860901\n",
      "80: 0.07637352919688899\n",
      "81: 0.07616432372350297\n",
      "82: 0.07595877132383178\n",
      "83: 0.0757567642864387\n",
      "84: 0.07555819933397771\n",
      "85: 0.07536297739013166\n",
      "86: 0.07517100336137382\n",
      "87: 0.07498218593245484\n",
      "88: 0.07479643737460291\n",
      "89: 0.07461367336551737\n",
      "90: 0.07443381282030484\n",
      "91: 0.0742567777325824\n",
      "92: 0.0740824930250326\n",
      "93: 0.0739108864087516\n",
      "94: 0.07374188825078681\n",
      "95: 0.07357543144930413\n",
      "96: 0.07341145131587304\n",
      "97: 0.07324988546439196\n",
      "98: 0.07309067370621734\n",
      "99: 0.0729337579510884\n",
      "GD High:\n",
      "0: 0.6931471805599468\n",
      "1: 0.21090224502932178\n",
      "2: 0.12200805944388637\n",
      "3: 0.09575341994009795\n",
      "4: 0.08975651302216298\n",
      "5: 0.08622367283676172\n",
      "6: 0.08378987657121471\n",
      "7: 0.08195217468082343\n",
      "8: 0.0804748574392759\n",
      "9: 0.07923489730811417\n",
      "10: 0.07816233416663747\n",
      "11: 0.07721416471758698\n",
      "12: 0.07636211344136304\n",
      "13: 0.07558655880482938\n",
      "14: 0.074873318515086\n",
      "15: 0.07421182774987053\n",
      "16: 0.07359403511854734\n",
      "17: 0.07301369267765086\n",
      "18: 0.07246587702547776\n",
      "19: 0.07194665449019169\n",
      "20: 0.0714528409434311\n",
      "21: 0.07098182629901179\n",
      "22: 0.07053144454803069\n",
      "23: 0.07009987652569041\n",
      "24: 0.06968557655526392\n",
      "25: 0.06928721669662435\n",
      "26: 0.0689036440797503\n",
      "27: 0.06853384802746261\n",
      "28: 0.0681769345432011\n",
      "29: 0.06783210636901384\n",
      "30: 0.06749864727789692\n",
      "31: 0.06717590960172047\n",
      "32: 0.06686330424490745\n",
      "33: 0.06656029261866397\n",
      "34: 0.06626638006801532\n",
      "35: 0.06598111046657938\n",
      "36: 0.06570406173097307\n",
      "37: 0.06543484206461105\n",
      "38: 0.06517308678431678\n",
      "39: 0.06491845561622239\n",
      "40: 0.06467063037254357\n",
      "41: 0.06442931293998022\n",
      "42: 0.06419422352516237\n",
      "43: 0.06396509911385079\n",
      "44: 0.06374169210932186\n",
      "45: 0.06352376912214058\n",
      "46: 0.06331110988881296\n",
      "47: 0.06310350630095476\n",
      "48: 0.06290076152989138\n",
      "49: 0.06270268923419912\n",
      "50: 0.0625091128397769\n",
      "51: 0.06231986488370425\n",
      "52: 0.06213478641449263\n",
      "53: 0.06195372644243421\n",
      "54: 0.06177654143465678\n",
      "55: 0.06160309485023406\n",
      "56: 0.06143325671131909\n",
      "57: 0.06126690320678562\n",
      "58: 0.061103916325296534\n",
      "59: 0.06094418351508704\n",
      "60: 0.06078759736806083\n",
      "61: 0.06063405532607128\n",
      "62: 0.06048345940748583\n",
      "63: 0.06033571595233522\n",
      "64: 0.06019073538452274\n",
      "65: 0.060048431989720284\n",
      "66: 0.05990872370771311\n",
      "67: 0.05977153193807229\n",
      "68: 0.05963678135813818\n",
      "69: 0.05950439975239121\n",
      "70: 0.05937431785236947\n",
      "71: 0.05924646918636391\n",
      "72: 0.0591207899381917\n",
      "73: 0.05899721881440399\n",
      "74: 0.058875696919340754\n",
      "75: 0.05875616763749037\n",
      "76: 0.05863857652265942\n",
      "77: 0.05852287119349313\n",
      "78: 0.0584090012349262\n",
      "79: 0.05829691810517383\n",
      "80: 0.058186575047905476\n",
      "81: 0.05807792700926596\n",
      "82: 0.05797093055943994\n",
      "83: 0.05786554381847097\n",
      "84: 0.05776172638607393\n",
      "85: 0.057659439275193035\n",
      "86: 0.0575586448490783\n",
      "87: 0.057459306761668535\n",
      "88: 0.05736138990108226\n",
      "89: 0.057264860336033944\n",
      "90: 0.05716968526500322\n",
      "91: 0.057075832967996384\n",
      "92: 0.05698327276075281\n",
      "93: 0.05689197495125358\n",
      "94: 0.056801910798404485\n",
      "95: 0.056713052472769414\n",
      "96: 0.05662537301923968\n",
      "97: 0.05653884632153307\n",
      "98: 0.05645344706842061\n",
      "99: 0.056369150721586915\n",
      "GD Reg Low:\n",
      "0: 0.6931471805599468\n",
      "1: 0.19129198084997467\n",
      "2: 0.1526758502825698\n",
      "3: 0.13760862030079862\n",
      "4: 0.13109956471430712\n",
      "5: 0.12686919689185994\n",
      "6: 0.12366479056854204\n",
      "7: 0.12111409112352993\n",
      "8: 0.11904001616230728\n",
      "9: 0.11733567206599359\n",
      "10: 0.11592841458059946\n",
      "11: 0.1147656241698358\n",
      "12: 0.11380768831956652\n",
      "13: 0.11302399710102719\n",
      "14: 0.1123904336866437\n",
      "15: 0.1118877022892911\n",
      "16: 0.11150016628246603\n",
      "17: 0.11121501582373668\n",
      "18: 0.11102165738573347\n",
      "19: 0.11091125748608206\n",
      "20: 0.11087639622263008\n",
      "21: 0.11091080057467766\n",
      "22: 0.11100913662786205\n",
      "23: 0.11116684596041394\n",
      "24: 0.11138001555110918\n",
      "25: 0.11164527342396745\n",
      "26: 0.11195970425723775\n",
      "27: 0.1123207806251798\n",
      "28: 0.11272630658720476\n",
      "29: 0.11317437110771013\n",
      "30: 0.113663309361307\n",
      "31: 0.11419167040711509\n",
      "32: 0.11475819004094466\n",
      "33: 0.11536176788277354\n",
      "34: 0.1160014479485326\n",
      "35: 0.11667640210402769\n",
      "36: 0.11738591591522589\n",
      "37: 0.11812937650079436\n",
      "38: 0.11890626206541348\n",
      "39: 0.119716132850285\n",
      "40: 0.1205586232836784\n",
      "41: 0.12143343515177003\n",
      "42: 0.12234033164034003\n",
      "43: 0.12327913212256698\n",
      "44: 0.12424970758834163\n",
      "45: 0.12525197662709517\n",
      "46: 0.12628590188981936\n",
      "47: 0.12735148696728119\n",
      "48: 0.12844877363085677\n",
      "49: 0.12957783939027084\n",
      "50: 0.13073879532911964\n",
      "51: 0.131931784184587\n",
      "52: 0.13315697864243622\n",
      "53: 0.13441457982231572\n",
      "54: 0.1357048159317593\n",
      "55: 0.13702794107013103\n",
      "56: 0.13838423416619236\n",
      "57: 0.1397739980350603\n",
      "58: 0.1411975585421163\n",
      "59: 0.14265526386296815\n",
      "60: 0.14414748382990125\n",
      "61: 0.14567460935639878\n",
      "62: 0.14723705193232034\n",
      "63: 0.14883524318318633\n",
      "64: 0.15046963448777925\n",
      "65: 0.1521406966489293\n",
      "66: 0.15384891961293168\n",
      "67: 0.1555948122335456\n",
      "68: 0.15737890207698169\n",
      "69: 0.15920173526465822\n",
      "70: 0.16106387635087233\n",
      "71: 0.16296590823282509\n",
      "72: 0.16490843209071382\n",
      "73: 0.16689206735584933\n",
      "74: 0.1689174517049617\n",
      "75: 0.1709852410790575\n",
      "76: 0.1730961097253551\n",
      "77: 0.1752507502609769\n",
      "78: 0.17744987375721705\n",
      "79: 0.17969420984332302\n",
      "80: 0.18198450682883557\n",
      "81: 0.18432153184363648\n",
      "82: 0.18670607099493952\n",
      "83: 0.1891389295405378\n",
      "84: 0.19162093207769665\n",
      "85: 0.1941529227471495\n",
      "86: 0.19673576545170388\n",
      "87: 0.19937034408903154\n",
      "88: 0.202057562798259\n",
      "89: 0.2047983462200195\n",
      "90: 0.2075936397696723\n",
      "91: 0.21044440992343247\n",
      "92: 0.2133516445171832\n",
      "93: 0.21631635305778946\n",
      "94: 0.2193395670467365\n",
      "95: 0.22242234031597446\n",
      "96: 0.22556574937584836\n",
      "97: 0.22877089377503135\n",
      "98: 0.23203889647239817\n",
      "99: 0.23537090422079457\n",
      "GD Reg High:\n",
      "0: 0.6931471805599468\n",
      "1: 0.181665732321542\n",
      "2: 0.1427334944812498\n",
      "3: 0.12752102517037422\n",
      "4: 0.12058748460555956\n",
      "5: 0.115760083985289\n",
      "6: 0.11187604972346239\n",
      "7: 0.10859736743195383\n",
      "8: 0.1057634506720609\n",
      "9: 0.10327624911915242\n",
      "10: 0.10106813951447549\n",
      "11: 0.09908945212128166\n",
      "12: 0.0973023342580507\n",
      "13: 0.09567721889341436\n",
      "14: 0.09419058242941725\n",
      "15: 0.09282343039207643\n",
      "16: 0.09156023228266284\n",
      "17: 0.09038815053411683\n",
      "18: 0.08929647008505232\n",
      "19: 0.08827616892442999\n",
      "20: 0.08731958997749709\n",
      "21: 0.08642018719709173\n",
      "22: 0.08557232683777227\n",
      "23: 0.08477113032087051\n",
      "24: 0.0840123488214467\n",
      "25: 0.08329226231069288\n",
      "26: 0.08260759763707219\n",
      "27: 0.08195546156318334\n",
      "28: 0.08133328564928413\n",
      "29: 0.08073878059382412\n",
      "30: 0.08016989817835068\n",
      "31: 0.07962479936884184\n",
      "32: 0.07910182743327013\n",
      "33: 0.07859948517113088\n",
      "34: 0.07811641553299847\n",
      "35: 0.0776513850500824\n",
      "36: 0.07720326960498142\n",
      "37: 0.07677104216258199\n",
      "38: 0.0763537621496967\n",
      "39: 0.07595056622764897\n",
      "40: 0.07556066024666834\n",
      "41: 0.0751833122069986\n",
      "42: 0.07481784608086756\n",
      "43: 0.07446363637330604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44: 0.07412010331933164\n",
      "45: 0.07378670863107543\n",
      "46: 0.073462951721705\n",
      "47: 0.07314836634400776\n",
      "48: 0.07284251759066726\n",
      "49: 0.07254499921093892\n",
      "50: 0.0722554312048559\n",
      "51: 0.07197345766152195\n",
      "52: 0.07169874481261863\n",
      "53: 0.07143097927613704\n",
      "54: 0.07116986646863992\n",
      "55: 0.07091512916717704\n",
      "56: 0.07066650620437775\n",
      "57: 0.07042375128231457\n",
      "58: 0.0701866318925011\n",
      "59: 0.06995492833092293\n",
      "60: 0.06972843279832183\n",
      "61: 0.0695069485771\n",
      "62: 0.0692902892772122\n",
      "63: 0.06907827814427348\n",
      "64: 0.06887074742387721\n",
      "65: 0.06866753777677441\n",
      "66: 0.06846849774014688\n",
      "67: 0.06827348323071981\n",
      "68: 0.06808235708590599\n",
      "69: 0.06789498863956907\n",
      "70: 0.0677112533293456\n",
      "71: 0.06753103233277329\n",
      "72: 0.06735421222974702\n",
      "73: 0.0671806846890697\n",
      "74: 0.06701034617707985\n",
      "75: 0.06684309768653204\n",
      "76: 0.06667884448407968\n",
      "77: 0.06651749587486094\n",
      "78: 0.06635896498282975\n",
      "79: 0.06620316854559516\n",
      "80: 0.06605002672264525\n",
      "81: 0.06589946291592953\n",
      "82: 0.06575140360186642\n",
      "83: 0.06560577817391922\n",
      "84: 0.06546251879496312\n",
      "85: 0.0653215602587255\n",
      "86: 0.06518283985964564\n",
      "87: 0.06504629727055329\n",
      "88: 0.06491187442761438\n",
      "89: 0.06477951542203594\n",
      "90: 0.06464916639806467\n",
      "91: 0.06452077545684913\n",
      "92: 0.06439429256576941\n",
      "93: 0.06426966947286948\n",
      "94: 0.06414685962605408\n",
      "95: 0.0640258180967389\n",
      "96: 0.06390650150766602\n",
      "97: 0.063788867964616\n",
      "98: 0.06367287699177218\n",
      "99: 0.06355848947050449\n",
      "Stochastic Gradient Descent:\n",
      "SGD Low:\n",
      "0: 0.13441446844421778\n",
      "1: 0.20832186872552624\n",
      "2: 0.1933736995443879\n",
      "3: 0.14547060946038948\n",
      "4: 0.12224294139763865\n",
      "5: 0.11530723056213665\n",
      "6: 0.07545099944867147\n",
      "7: 0.08656625551151013\n",
      "8: 0.1685771713430852\n",
      "9: 0.15639394678591936\n",
      "10: 0.06814565526092242\n",
      "11: 0.12049711785133153\n",
      "12: 0.12992862377302106\n",
      "13: 0.1405722519286286\n",
      "14: 0.12187138855581035\n",
      "15: 0.15036630579574609\n",
      "16: 0.10659028626828254\n",
      "17: 0.11063717854600123\n",
      "18: 0.14042323774439847\n",
      "19: 0.11634589340326494\n",
      "20: 0.17071248742256914\n",
      "21: 0.10709666320993393\n",
      "22: 0.08819495690446717\n",
      "23: 0.11622247974044714\n",
      "24: 0.09942188808966923\n",
      "25: 0.08991222102773959\n",
      "26: 0.06933245754798283\n",
      "27: 0.08997523369372068\n",
      "28: 0.09198015421723944\n",
      "29: 0.16842785805695812\n",
      "30: 0.11615923022562208\n",
      "31: 0.12048314271326054\n",
      "32: 0.11305513203976666\n",
      "33: 0.0879732576293511\n",
      "34: 0.16746092088579861\n",
      "35: 0.11480980813043375\n",
      "36: 0.09030895778602509\n",
      "37: 0.1276851379270776\n",
      "38: 0.12551367814002454\n",
      "39: 0.11207242930217871\n",
      "40: 0.11192558214302566\n",
      "41: 0.11700681782221177\n",
      "42: 0.08167356276190034\n",
      "43: 0.08444586137643686\n",
      "44: 0.096598485654221\n",
      "45: 0.07701333474780904\n",
      "46: 0.08628756852420728\n",
      "47: 0.1539925941733271\n",
      "48: 0.09665647559365916\n",
      "49: 0.08278440119709364\n",
      "50: 0.10969730739830201\n",
      "51: 0.11693923110813186\n",
      "52: 0.10612467504043514\n",
      "53: 0.11943235395007222\n",
      "54: 0.09318111888414084\n",
      "55: 0.10299148115616184\n",
      "56: 0.1047696138413598\n",
      "57: 0.12401027747192507\n",
      "58: 0.08319593105863185\n",
      "59: 0.13992841912076484\n",
      "60: 0.1230425690200727\n",
      "61: 0.10010216002102065\n",
      "62: 0.09718675658259505\n",
      "63: 0.0856262228719477\n",
      "64: 0.06509166181084979\n",
      "65: 0.065158828146862\n",
      "66: 0.06931623777583062\n",
      "67: 0.08744463809445348\n",
      "68: 0.07293306070005676\n",
      "69: 0.10179232166983203\n",
      "70: 0.08877420148178461\n",
      "71: 0.0854834704782546\n",
      "72: 0.06700400948966874\n",
      "73: 0.07852256614825984\n",
      "74: 0.07065009097521764\n",
      "75: 0.18946216525751283\n",
      "76: 0.09349632091114768\n",
      "77: 0.11883379422065497\n",
      "78: 0.0765680323274646\n",
      "79: 0.08988910118316788\n",
      "80: 0.09131180677707154\n",
      "81: 0.07305299979464226\n",
      "82: 0.09346006752237279\n",
      "83: 0.11009791745601263\n",
      "84: 0.079161607738841\n",
      "85: 0.08461123893793614\n",
      "86: 0.05773291653553072\n",
      "87: 0.06700738328325681\n",
      "88: 0.07239660803224664\n",
      "89: 0.07246201661146068\n",
      "90: 0.06224891808128645\n",
      "91: 0.10908488188255472\n",
      "92: 0.11748009381392349\n",
      "93: 0.12431610198520847\n",
      "94: 0.09243568632808587\n",
      "95: 0.1350557444618192\n",
      "96: 0.07596404810167842\n",
      "97: 0.08946942091803645\n",
      "98: 0.06236041614671878\n",
      "99: 0.07186632546447846\n",
      "SGD High:\n",
      "0: 0.6719870321983521\n",
      "1: 0.7268071794622653\n",
      "2: 0.8081607909313936\n",
      "3: 0.9330069817135788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_1896\\4766473.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  denominator = 1+numpy.exp(innerproduct)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4: 0.7777650339033636\n",
      "5: 0.518292601380117\n",
      "6: 0.6984435473606473\n",
      "7: 0.28893895786580753\n",
      "8: 0.2994656964774918\n",
      "9: 0.38058784904857806\n",
      "10: 0.3581953578132904\n",
      "11: 0.6080755397252429\n",
      "12: 0.41604848031724295\n",
      "13: 0.42153994372831144\n",
      "14: 0.42038520958244846\n",
      "15: 0.30160128501959244\n",
      "16: 0.40298522836255307\n",
      "17: 0.5517057779152572\n",
      "18: 0.6111371938966658\n",
      "19: 0.4588096443220545\n",
      "20: 0.46748278739886556\n",
      "21: 0.4507507358660054\n",
      "22: 0.7223226708372184\n",
      "23: 0.4129295261723573\n",
      "24: 0.4568969484350167\n",
      "25: 0.44545159741975177\n",
      "26: 0.7340612912291086\n",
      "27: 0.46335858850491335\n",
      "28: 0.3785373696077024\n",
      "29: 0.34203328099715835\n",
      "30: 0.3485310435147717\n",
      "31: 0.5461409524353095\n",
      "32: 0.39230962886889265\n",
      "33: 0.27250956649442926\n",
      "34: 0.3722685025913496\n",
      "35: 0.3828624818464857\n",
      "36: 0.2926327921548107\n",
      "37: 0.6141584738397593\n",
      "38: 0.2943413063313841\n",
      "39: 0.23005381840151784\n",
      "40: 0.5272466595296463\n",
      "41: 0.3554660366095594\n",
      "42: 0.25438886659784116\n",
      "43: 0.32954095537910955\n",
      "44: 0.4601063888769243\n",
      "45: 0.5862435641853287\n",
      "46: 0.388635619737888\n",
      "47: 0.4032417346644445\n",
      "48: 0.50795377910024\n",
      "49: 0.2818268188228026\n",
      "50: 0.4543995542091941\n",
      "51: 0.23741437570143262\n",
      "52: 0.3714811399704775\n",
      "53: 0.4858091883330615\n",
      "54: 0.514691674658234\n",
      "55: 0.28129576779327464\n",
      "56: 0.31992228963828623\n",
      "57: 0.40626905621122816\n",
      "58: 0.7889332211534998\n",
      "59: 0.37637258269709273\n",
      "60: 0.5517565527021427\n",
      "61: 0.34899515993835295\n",
      "62: 0.3480214916320393\n",
      "63: 0.383010759437453\n",
      "64: 0.24200474954713147\n",
      "65: 0.5517950899337937\n",
      "66: 0.7970018418724933\n",
      "67: 0.39045197592717795\n",
      "68: 0.31133494883801377\n",
      "69: 0.700900904639532\n",
      "70: 0.3699556613001201\n",
      "71: 0.342087909185539\n",
      "72: 0.2812465239637274\n",
      "73: 0.37496984837428443\n",
      "74: 0.48203294671624963\n",
      "75: 0.33837160399105254\n",
      "76: 0.370443519437686\n",
      "77: 0.3435559872568401\n",
      "78: 0.25683218905003136\n",
      "79: 0.2846182968102931\n",
      "80: 0.2802827367478204\n",
      "81: 0.3903917952378251\n",
      "82: 0.13775142450679312\n",
      "83: 0.7352113570219073\n",
      "84: 0.3756309817786813\n",
      "85: 0.3362553215274332\n",
      "86: 0.338571043954188\n",
      "87: 0.24866714856319186\n",
      "88: 0.2901700074193245\n",
      "89: 0.33268699530132\n",
      "90: 0.560125448348823\n",
      "91: 0.31983937164226034\n",
      "92: 0.37554019767449304\n",
      "93: 0.4173725616239511\n",
      "94: 0.4593529968903365\n",
      "95: 0.28803617740872883\n",
      "96: 0.26141580068711756\n",
      "97: 0.3487933890996861\n",
      "98: 0.3747419807264613\n",
      "99: 0.36247236607341043\n",
      "SGD Reg Low:\n",
      "0: 0.6022000928192085\n",
      "1: 0.6669413617706053\n",
      "2: 0.83428315064833\n",
      "3: 0.6883721456535613\n",
      "4: 0.7177076618407099\n",
      "5: 0.5642479974209347\n",
      "6: 0.9752891235993919\n",
      "7: 0.9960825831376892\n",
      "8: 0.6669659480306355\n",
      "9: 0.7002256509934633\n",
      "10: 0.637554908492819\n",
      "11: 0.6139258565199287\n",
      "12: 0.5605530494610653\n",
      "13: 0.708463440347896\n",
      "14: 0.7389124221176967\n",
      "15: 0.4446542666345249\n",
      "16: 0.43470081427742135\n",
      "17: 0.5832807743503122\n",
      "18: 0.5822457130998471\n",
      "19: 0.5117639917397032\n",
      "20: 0.27192520098679057\n",
      "21: 0.6126665425719389\n",
      "22: 0.8187460178422058\n",
      "23: 0.6584922619308112\n",
      "24: 0.7150491238811759\n",
      "25: 0.729996451576503\n",
      "26: 0.4647425441673265\n",
      "27: 0.891110508400294\n",
      "28: 0.44120313476689266\n",
      "29: 0.781409642715134\n",
      "30: 0.6942543930739176\n",
      "31: 0.6936618909907789\n",
      "32: 0.7353486998098394\n",
      "33: 0.5775644288501386\n",
      "34: 0.5570403111338218\n",
      "35: 0.8079004669414948\n",
      "36: 0.4567659522466196\n",
      "37: 0.5487358001087228\n",
      "38: 0.6996475937819149\n",
      "39: 0.9282724847992715\n",
      "40: 0.3574010362618805\n",
      "41: 0.4138965689002245\n",
      "42: 0.8343316360756823\n",
      "43: 0.7742144221661257\n",
      "44: 0.4008765508142054\n",
      "45: 0.3352305041196327\n",
      "46: 0.7927886578952669\n",
      "47: 0.7528893261287574\n",
      "48: 0.5803111855268817\n",
      "49: 0.39876394752136424\n",
      "50: 0.49852135443054024\n",
      "51: 0.9703622406056109\n",
      "52: 0.4717726943888807\n",
      "53: 0.677778482065786\n",
      "54: 0.4021592644987534\n",
      "55: 0.5920657078241194\n",
      "56: 0.3614380473169894\n",
      "57: 0.6312695134888768\n",
      "58: 0.5051810031865224\n",
      "59: 0.7704061644392536\n",
      "60: 0.5586241672913971\n",
      "61: 0.8708097237557896\n",
      "62: 0.9171958840283696\n",
      "63: 0.8352108641656003\n",
      "64: 0.6278647641917393\n",
      "65: 0.7332951749400486\n",
      "66: 0.6624012962449516\n",
      "67: 0.4156885294994539\n",
      "68: 0.5910315853474296\n",
      "69: 0.8859470908840918\n",
      "70: 0.6404689861724304\n",
      "71: 0.9330146764250516\n",
      "72: 0.9871290285821525\n",
      "73: 0.5929368824022629\n",
      "74: 0.699745567212992\n",
      "75: 0.8437388387547197\n",
      "76: 0.7543581731836204\n",
      "77: 0.8989759775698299\n",
      "78: 0.8064034939627601\n",
      "79: 1.0218554366331074\n",
      "80: 0.7500439382309515\n",
      "81: 0.9921097162756357\n",
      "82: 0.6533824682668108\n",
      "83: 0.8327199809415093\n",
      "84: 0.6383423589729342\n",
      "85: 0.845924696751519\n",
      "86: 0.7723595817769618\n",
      "87: 0.8558209608232444\n",
      "88: 0.6836064748630463\n",
      "89: 0.776463439878188\n",
      "90: 0.5470501905929769\n",
      "91: 0.3952676226103248\n",
      "92: 0.5233097802712249\n",
      "93: 0.8120966655392615\n",
      "94: 0.948035260513321\n",
      "95: 0.9373653557905263\n",
      "96: 0.762461316462376\n",
      "97: 0.9659109785151179\n",
      "98: 0.7077004094839772\n",
      "99: 0.6365851012258253\n",
      "SGD Reg High:\n",
      "0: 0.5802380354065324\n",
      "1: 0.36028491575402405\n",
      "2: 0.4268626359384396\n",
      "3: 0.411017721807572\n",
      "4: 0.3064613034772062\n",
      "5: 0.34136439148424674\n",
      "6: 0.26781202832398415\n",
      "7: 0.32853709844766\n",
      "8: 0.3916864034085096\n",
      "9: 0.23418809947550928\n",
      "10: 0.18933005770776967\n",
      "11: 0.3237564143488559\n",
      "12: 0.21745194741434118\n",
      "13: 0.19655791665221922\n",
      "14: 0.5230377738137335\n",
      "15: 0.22363725409452928\n",
      "16: 0.31139029561698944\n",
      "17: 0.1566526629610486\n",
      "18: 0.29351682107839844\n",
      "19: 0.2702037421965135\n",
      "20: 0.253197437306504\n",
      "21: 0.2684249388036354\n",
      "22: 0.2077909950076395\n",
      "23: 0.2961673227583397\n",
      "24: 0.21634108790472648\n",
      "25: 0.2472325138110666\n",
      "26: 0.15477623937239898\n",
      "27: 0.16928014303524933\n",
      "28: 0.157798372265311\n",
      "29: 0.1557131378883708\n",
      "30: 0.18238385422662678\n",
      "31: 0.20037262667634823\n",
      "32: 0.232213213257186\n",
      "33: 0.19401143958013856\n",
      "34: 0.21417306980549572\n",
      "35: 0.16513894016947434\n",
      "36: 0.3126794819175529\n",
      "37: 0.2598523319965413\n",
      "38: 0.22261525901704074\n",
      "39: 0.22283245704731833\n",
      "40: 0.29512190859092763\n",
      "41: 0.3274876353080633\n",
      "42: 0.21858369389837348\n",
      "43: 0.27168651539313654\n",
      "44: 0.23329197724364703\n",
      "45: 0.15270916246371807\n",
      "46: 0.23277721070708918\n",
      "47: 0.3825817585947637\n",
      "48: 0.21211126229396146\n",
      "49: 0.26867216063219\n",
      "50: 0.20996089014887906\n",
      "51: 0.22903859275396665\n",
      "52: 0.28545606883671576\n",
      "53: 0.15360863594192703\n",
      "54: 0.22700422104333803\n",
      "55: 0.1718366691526686\n",
      "56: 0.13980558770892132\n",
      "57: 0.15831577937717928\n",
      "58: 0.2203859315940561\n",
      "59: 0.17865142790801392\n",
      "60: 0.15041766275745228\n",
      "61: 0.1370294722584194\n",
      "62: 0.12640795500281582\n",
      "63: 0.19000077393293063\n",
      "64: 0.23421360589589515\n",
      "65: 0.3668965173039792\n",
      "66: 0.20425027923754888\n",
      "67: 0.2731144147309269\n",
      "68: 0.19801132684690476\n",
      "69: 0.15780190066534971\n",
      "70: 0.1645790646401329\n",
      "71: 0.10091944220767718\n",
      "72: 0.4056422185144382\n",
      "73: 0.19878091787725424\n",
      "74: 0.17392757506228929\n",
      "75: 0.17313777627392055\n",
      "76: 0.1506934800157014\n",
      "77: 0.2693969529362912\n",
      "78: 0.1881209254402329\n",
      "79: 0.1605929876514389\n",
      "80: 0.2095729692707629\n",
      "81: 0.1485329226726055\n",
      "82: 0.13674587092640988\n",
      "83: 0.3459777846480139\n",
      "84: 0.265627951885065\n",
      "85: 0.2009250152192836\n",
      "86: 0.2192491221880703\n",
      "87: 0.21509417632329067\n",
      "88: 0.2343443738253826\n",
      "89: 0.1689022097631432\n",
      "90: 0.19348888715841348\n",
      "91: 0.2119754677618773\n",
      "92: 0.20370951751287275\n",
      "93: 0.21127558337044053\n",
      "94: 0.19575691996851036\n",
      "95: 0.29113212092656726\n",
      "96: 0.17801860392307492\n",
      "97: 0.24320071868981644\n",
      "98: 0.15207298991404147\n",
      "99: 0.14516289376814978\n",
      "Mini Batch Gradient Descent:\n",
      "MBGD Low:\n",
      "0: 0.12165316325611147\n",
      "1: 0.08497012037844254\n",
      "2: 0.070772957297929\n",
      "3: 0.07169902268077417\n",
      "4: 0.06144800898967208\n",
      "5: 0.06334931282298253\n",
      "6: 0.05876346401698997\n",
      "7: 0.06250380248337545\n",
      "8: 0.05911091125999443\n",
      "9: 0.06132550035455222\n",
      "10: 0.05785189119657958\n",
      "11: 0.05974846466019576\n",
      "12: 0.052991568647007804\n",
      "13: 0.05759967630435628\n",
      "14: 0.059422390649867755\n",
      "15: 0.057857236194453834\n",
      "16: 0.058747497415104315\n",
      "17: 0.05505278392317932\n",
      "18: 0.05289829782381773\n",
      "19: 0.054071535825539564\n",
      "20: 0.053854745055942954\n",
      "21: 0.05387706192981487\n",
      "22: 0.05401444190561743\n",
      "23: 0.055975210039765216\n",
      "24: 0.05691501305117662\n",
      "25: 0.0520443503915856\n",
      "26: 0.051030921186485396\n",
      "27: 0.05143749273561762\n",
      "28: 0.050212899739536646\n",
      "29: 0.054596266424980035\n",
      "30: 0.050828239808536034\n",
      "31: 0.050309666098152156\n",
      "32: 0.05426862380866512\n",
      "33: 0.055342181075117916\n",
      "34: 0.051809256042133954\n",
      "35: 0.0514511645439848\n",
      "36: 0.052468760198463395\n",
      "37: 0.05049064408958221\n",
      "38: 0.05851871392359693\n",
      "39: 0.051472782211319325\n",
      "40: 0.04623977401641478\n",
      "41: 0.05382094294936033\n",
      "42: 0.049587690915280584\n",
      "43: 0.051735533685913014\n",
      "44: 0.052578507301435956\n",
      "45: 0.0486061284445977\n",
      "46: 0.051003151370940136\n",
      "47: 0.048501028427152815\n",
      "48: 0.05456627068054294\n",
      "49: 0.05379529284896318\n",
      "50: 0.04826395454246686\n",
      "51: 0.048715343409209105\n",
      "52: 0.04964857409442548\n",
      "53: 0.048946896200610636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54: 0.04970615757450171\n",
      "55: 0.050926648613685316\n",
      "56: 0.048813101652176456\n",
      "57: 0.04924068540675733\n",
      "58: 0.048535098481289424\n",
      "59: 0.047793329067881515\n",
      "60: 0.04737731442350183\n",
      "61: 0.04778078950268336\n",
      "62: 0.048768624914687614\n",
      "63: 0.0507419414983769\n",
      "64: 0.05031024058963816\n",
      "65: 0.04886039311849885\n",
      "66: 0.04878727247279865\n",
      "67: 0.04815279900839393\n",
      "68: 0.04641843548512054\n",
      "69: 0.04798040734424493\n",
      "70: 0.04774362884655795\n",
      "71: 0.04616281442171634\n",
      "72: 0.047888529551117234\n",
      "73: 0.04696503010277427\n",
      "74: 0.04722877061042787\n",
      "75: 0.045637386045518855\n",
      "76: 0.05053661507436794\n",
      "77: 0.04979514672037391\n",
      "78: 0.04729406321459381\n",
      "79: 0.04813218422900313\n",
      "80: 0.04747225845383364\n",
      "81: 0.04524356103460408\n",
      "82: 0.04703128307877201\n",
      "83: 0.04711332124488516\n",
      "84: 0.046069814925067766\n",
      "85: 0.046651138976949405\n",
      "86: 0.04643681617685324\n",
      "87: 0.04500063637790598\n",
      "88: 0.04731403092630021\n",
      "89: 0.04692867646111637\n",
      "90: 0.04523016522463367\n",
      "91: 0.04258639772590615\n",
      "92: 0.04612750916224267\n",
      "93: 0.04629097119172293\n",
      "94: 0.045720885218051524\n",
      "95: 0.04559312222923421\n",
      "96: 0.045838377210477566\n",
      "97: 0.04570194891477358\n",
      "98: 0.04503653532107381\n",
      "99: 0.04800467532146167\n",
      "MBGD High:\n",
      "0: 0.10582668740686053\n",
      "1: 0.20069786413703442\n",
      "2: 0.09021538730738897\n",
      "3: 0.10167077628810522\n",
      "4: 0.18627656777172055\n",
      "5: 0.15300577807630467\n",
      "6: 0.10447880654763363\n",
      "7: 0.12205157876352365\n",
      "8: 0.1368082377389347\n",
      "9: 0.09191248892231461\n",
      "10: 0.08332125185922162\n",
      "11: 0.09401890756713892\n",
      "12: 0.1344155866569631\n",
      "13: 0.12184075856582074\n",
      "14: 0.15252044305692264\n",
      "15: 0.08141324107494245\n",
      "16: 0.07499825352711652\n",
      "17: 0.08630556134280805\n",
      "18: 0.12598241899658938\n",
      "19: 0.0886981080610931\n",
      "20: 0.12133621302198193\n",
      "21: 0.08595606325530816\n",
      "22: 0.1174339093091537\n",
      "23: 0.08977773605817663\n",
      "24: 0.10053317496734702\n",
      "25: 0.08219454036431567\n",
      "26: 0.08058576606198188\n",
      "27: 0.1284214352972885\n",
      "28: 0.09688607299959724\n",
      "29: 0.07353297441665926\n",
      "30: 0.09092910550759185\n",
      "31: 0.09768495386535601\n",
      "32: 0.07425314482692189\n",
      "33: 0.08636044088401094\n",
      "34: 0.08398822922028067\n",
      "35: 0.08870044808065454\n",
      "36: 0.07872030209117636\n",
      "37: 0.16312053932037426\n",
      "38: 0.09849480863055197\n",
      "39: 0.09225970779701415\n",
      "40: 0.07409739882078752\n",
      "41: 0.09826064528361697\n",
      "42: 0.08135498289767501\n",
      "43: 0.11352304674639711\n",
      "44: 0.07645029604318436\n",
      "45: 0.08045352014898564\n",
      "46: 0.11045670918848781\n",
      "47: 0.08647463090986139\n",
      "48: 0.0851073796103024\n",
      "49: 0.07482454316248777\n",
      "50: 0.05576476464461324\n",
      "51: 0.05744172058556866\n",
      "52: 0.08425562681649397\n",
      "53: 0.08582002181398984\n",
      "54: 0.14410132874491544\n",
      "55: 0.1215982809088361\n",
      "56: 0.09679103530426826\n",
      "57: 0.06346304553012787\n",
      "58: 0.08822453156768896\n",
      "59: 0.08218983215575346\n",
      "60: 0.07808522583212135\n",
      "61: 0.06829483125190129\n",
      "62: 0.06707152422206843\n",
      "63: 0.061245307563163126\n",
      "64: 0.06874535198181748\n",
      "65: 0.06530306822828891\n",
      "66: 0.09117876237892221\n",
      "67: 0.06782423370471201\n",
      "68: 0.06469715191202514\n",
      "69: 0.06359364973939437\n",
      "70: 0.09083259316642939\n",
      "71: 0.08994030517368808\n",
      "72: 0.09860227987033078\n",
      "73: 0.08603048733704874\n",
      "74: 0.07072947599900783\n",
      "75: 0.08273122648565868\n",
      "76: 0.07498436567853534\n",
      "77: 0.1040619594975918\n",
      "78: 0.1079149594820632\n",
      "79: 0.11305779398993346\n",
      "80: 0.06826588410880717\n",
      "81: 0.05542495215933202\n",
      "82: 0.06592567793253279\n",
      "83: 0.06063817096093604\n",
      "84: 0.05722321903351016\n",
      "85: 0.06759775033483323\n",
      "86: 0.06277679713581666\n",
      "87: 0.08816103959758263\n",
      "88: 0.12057739148372414\n",
      "89: 0.08775312683201135\n",
      "90: 0.06508173614123296\n",
      "91: 0.05843039788998145\n",
      "92: 0.09858926214235376\n",
      "93: 0.07015829643167239\n",
      "94: 0.07089423952089195\n",
      "95: 0.11361089792328356\n",
      "96: 0.08363962419706854\n",
      "97: 0.07167216534302437\n",
      "98: 0.05863077460186341\n",
      "99: 0.058046460072263315\n",
      "MBGD Reg Low:\n",
      "0: 0.17779059229412347\n",
      "1: 0.13285212652283412\n",
      "2: 0.1414078233707812\n",
      "3: 0.14230628652798238\n",
      "4: 0.13383890229827597\n",
      "5: 0.12949978438384654\n",
      "6: 0.17139707406166776\n",
      "7: 0.16887646956250138\n",
      "8: 0.13012519123793906\n",
      "9: 0.14104727712017165\n",
      "10: 0.1390290363324931\n",
      "11: 0.13760359701945624\n",
      "12: 0.15518611539549299\n",
      "13: 0.12675663709317211\n",
      "14: 0.14674202424985974\n",
      "15: 0.14271718826266788\n",
      "16: 0.14417787298692483\n",
      "17: 0.13806814078507074\n",
      "18: 0.1445435852673594\n",
      "19: 0.16220818737150103\n",
      "20: 0.14845805965864975\n",
      "21: 0.16229197732830983\n",
      "22: 0.21123891188998908\n",
      "23: 0.13973663095607372\n",
      "24: 0.16516905118659794\n",
      "25: 0.16114739681953377\n",
      "26: 0.14513896052188272\n",
      "27: 0.13504378568355416\n",
      "28: 0.12749032819713552\n",
      "29: 0.1259349808088914\n",
      "30: 0.16598648619144935\n",
      "31: 0.1787499035444153\n",
      "32: 0.15418340929257288\n",
      "33: 0.14488131276292066\n",
      "34: 0.13509294331154933\n",
      "35: 0.14316915166167168\n",
      "36: 0.12476947516324981\n",
      "37: 0.13554474003329794\n",
      "38: 0.15113638723555747\n",
      "39: 0.12771642216840093\n",
      "40: 0.16012205338110966\n",
      "41: 0.1406932338145993\n",
      "42: 0.15435708772755546\n",
      "43: 0.1314009572492846\n",
      "44: 0.13236669847836194\n",
      "45: 0.18985855187698505\n",
      "46: 0.13144215021097327\n",
      "47: 0.15755197471969995\n",
      "48: 0.15938232961517043\n",
      "49: 0.15805101631839272\n",
      "50: 0.13449130531289413\n",
      "51: 0.1493960291392327\n",
      "52: 0.16287846608854537\n",
      "53: 0.14579874186902794\n",
      "54: 0.12620768304276803\n",
      "55: 0.19210456134472814\n",
      "56: 0.1409841281462807\n",
      "57: 0.14488104495317905\n",
      "58: 0.17731734770077\n",
      "59: 0.13951824749900982\n",
      "60: 0.1509512393847455\n",
      "61: 0.14998395817836074\n",
      "62: 0.1421985667293621\n",
      "63: 0.13552852607430205\n",
      "64: 0.13400538370619675\n",
      "65: 0.13903638789666525\n",
      "66: 0.14957345450754797\n",
      "67: 0.18981297796641353\n",
      "68: 0.12873232404880847\n",
      "69: 0.18196127994635547\n",
      "70: 0.13371185774319264\n",
      "71: 0.1487018408969564\n",
      "72: 0.15613446574992618\n",
      "73: 0.18014372064165968\n",
      "74: 0.17445257808484557\n",
      "75: 0.14085712176641302\n",
      "76: 0.133418399024948\n",
      "77: 0.16109785385055161\n",
      "78: 0.1332700714012085\n",
      "79: 0.1418858106901363\n",
      "80: 0.18132320477419706\n",
      "81: 0.15215194759737619\n",
      "82: 0.15731630808580774\n",
      "83: 0.19607496176517492\n",
      "84: 0.13734098758755448\n",
      "85: 0.1628170652858649\n",
      "86: 0.12441506633478767\n",
      "87: 0.13945974367718078\n",
      "88: 0.13398337425181572\n",
      "89: 0.14679691737895884\n",
      "90: 0.15738307319229963\n",
      "91: 0.20730634835140072\n",
      "92: 0.16281442405039157\n",
      "93: 0.13684598102974424\n",
      "94: 0.19866575549524054\n",
      "95: 0.16554882291056383\n",
      "96: 0.18039807797245883\n",
      "97: 0.14532637486468278\n",
      "98: 0.13574689318938016\n",
      "99: 0.1878158682554757\n",
      "MBGD Reg High:\n",
      "0: 0.16141806844349404\n",
      "1: 0.09452449553463817\n",
      "2: 0.07601556304622965\n",
      "3: 0.07907637450206235\n",
      "4: 0.09720101067998303\n",
      "5: 0.08016181364294665\n",
      "6: 0.07651372560433048\n",
      "7: 0.06526705939496434\n",
      "8: 0.07654175705490245\n",
      "9: 0.0593174894367864\n",
      "10: 0.06199563495872811\n",
      "11: 0.07216855702600125\n",
      "12: 0.057210437428937055\n",
      "13: 0.07071237480356261\n",
      "14: 0.07122288387573548\n",
      "15: 0.08121584314826566\n",
      "16: 0.06098171360564439\n",
      "17: 0.05562491782125296\n",
      "18: 0.06287170112994965\n",
      "19: 0.05750768931897771\n",
      "20: 0.06710606826766118\n",
      "21: 0.09841360864679549\n",
      "22: 0.06583030627062582\n",
      "23: 0.05984665964658955\n",
      "24: 0.05800329786484503\n",
      "25: 0.05336455218580557\n",
      "26: 0.0591172934439443\n",
      "27: 0.07244845587787174\n",
      "28: 0.06697945686126468\n",
      "29: 0.07042693208094938\n",
      "30: 0.05136345844272465\n",
      "31: 0.06327343782679602\n",
      "32: 0.056930819804118675\n",
      "33: 0.08432333686063061\n",
      "34: 0.06706345959506556\n",
      "35: 0.05730637655658701\n",
      "36: 0.050427939779701794\n",
      "37: 0.07615675538661147\n",
      "38: 0.058255815976632146\n",
      "39: 0.05098736629091008\n",
      "40: 0.055596942851123675\n",
      "41: 0.05261175714102368\n",
      "42: 0.05639379617740815\n",
      "43: 0.06880649353443206\n",
      "44: 0.07430529682305344\n",
      "45: 0.05274893699737676\n",
      "46: 0.054484106120331124\n",
      "47: 0.06001571930026572\n",
      "48: 0.06819209207787552\n",
      "49: 0.056349035597168334\n",
      "50: 0.050220407290779465\n",
      "51: 0.05933321089651578\n",
      "52: 0.05350093532443409\n",
      "53: 0.053554722552976\n",
      "54: 0.09384151882506417\n",
      "55: 0.05547119360955524\n",
      "56: 0.05811349920283246\n",
      "57: 0.056488708103579255\n",
      "58: 0.051044693705962534\n",
      "59: 0.06152668491980158\n",
      "60: 0.05952829378651471\n",
      "61: 0.054291432651360945\n",
      "62: 0.05628796918215165\n",
      "63: 0.05205907113486288\n",
      "64: 0.05463736591750249\n",
      "65: 0.0472632113527274\n",
      "66: 0.07080417263670853\n",
      "67: 0.05866061294184294\n",
      "68: 0.05628174338988868\n",
      "69: 0.05667469633954044\n",
      "70: 0.05818013682842011\n",
      "71: 0.05086541214446799\n",
      "72: 0.05376934031898835\n",
      "73: 0.0471364870410991\n",
      "74: 0.05400092758250922\n",
      "75: 0.04840674721005514\n",
      "76: 0.05379734606476047\n",
      "77: 0.06760227592915882\n",
      "78: 0.04722368778003464\n",
      "79: 0.050438261537598665\n",
      "80: 0.056834791902147054\n",
      "81: 0.07113942137451143\n",
      "82: 0.050028170165931696\n",
      "83: 0.05561065911799145\n",
      "84: 0.053299009785239194\n",
      "85: 0.05396016855506029\n",
      "86: 0.050192734990756084\n",
      "87: 0.051907497526627\n",
      "88: 0.048833938476953165\n",
      "89: 0.05163664868173892\n",
      "90: 0.06441180815423132\n",
      "91: 0.0683638691691705\n",
      "92: 0.07283243984851413\n",
      "93: 0.051576423244027676\n",
      "94: 0.044550342607156276\n",
      "95: 0.05592306274488532\n",
      "96: 0.049246310290523926\n",
      "97: 0.05241952310641034\n",
      "98: 0.04736407952765322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99: 0.063170921201504\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient Descent:\")\n",
    "print(\"GD Low:\")\n",
    "w_gd_low,obj_gd_low = gradient_descent(x_train,y_train,0,.5,None)\n",
    "print(\"GD High:\")\n",
    "w_gd_high,obj_gd_high = gradient_descent(x_train,y_train,0,2,None)\n",
    "print(\"GD Reg Low:\")\n",
    "w_gd_reg_low,obj_gd_reg_low = gradient_descent(x_train,y_train,1E-2,1,None)\n",
    "print(\"GD Reg High:\")\n",
    "w_gd_reg_high,obj_gd_reg_high = gradient_descent(x_train,y_train,1E-5,1,None)\n",
    "\n",
    "print(\"Stochastic Gradient Descent:\")\n",
    "print(\"SGD Low:\")\n",
    "w_sgd_low,obj_sgd_low = sgd(x_train,y_train,0,.5,None)\n",
    "print(\"SGD High:\")\n",
    "w_sgd_high,obj_sgd_high = sgd(x_train,y_train,0,2,None)\n",
    "print(\"SGD Reg Low:\")\n",
    "w_sgd_reg_low,obj_sgd_reg_low = sgd(x_train,y_train,1E-2,1,None)\n",
    "print(\"SGD Reg High:\")\n",
    "w_sgd_reg_high,obj_sgd_reg_high = sgd(x_train,y_train,1E-5,1,None)\n",
    "\n",
    "print(\"Mini Batch Gradient Descent:\")\n",
    "print(\"MBGD Low:\")\n",
    "w_mbgd_low,obj_mbgd_low = mbgd(x_train,y_train,0,.5,5,None)\n",
    "print(\"MBGD High:\")\n",
    "w_mbgd_high,obj_mbgd_high = mbgd(x_train,y_train,0,2,5,None)\n",
    "print(\"MBGD Reg Low:\")\n",
    "w_mbgd_reg_low,obj_mbgd_reg_low = mbgd(x_train,y_train,1E-2,1,5,None)\n",
    "print(\"MBGD Reg High:\")\n",
    "w_mbgd_reg_high,obj_mbgd_reg_high = mbgd(x_train,y_train,1E-5,1,5,None)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

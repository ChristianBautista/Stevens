{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ady8ZVZd30h4"
      },
      "source": [
        "# Assignment 2: Build a CNN for image recognition.\n",
        "\n",
        "## Due Date:  March 29, 11:59PM\n",
        "\n",
        "### Name: Christian Bautista\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtL3bNpm30h_"
      },
      "source": [
        "## Introduction:\n",
        "\n",
        "1. In this assignment, you will build Convolutional Neural Network to classify CIFAR-10 Images.\n",
        "2. You can directly load dataset from many deep learning packages.\n",
        "3. You can use any deep learning packages such as pytorch, keras or tensorflow for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZfwLE6830iA"
      },
      "source": [
        "## Requirements:\n",
        "\n",
        "1. You need to load cifar 10 data and split the entire training dataset into training and validation.\n",
        "2. You will implement a CNN model to classify cifar 10 images with provided structure.\n",
        "3. You need to plot the training and validation accuracy or loss obtained from above step.\n",
        "4. Then you can use tuned parameters to train using the entire training dataset.\n",
        "5. You should report the testing accuracy using the model with complete data.\n",
        "6. You may try to change the structure (e.g, add BN layer or dropout layer,...) and analyze your findings.\n",
        "\n",
        "## Google Colab\n",
        "\n",
        "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzhfVh4g30iB"
      },
      "source": [
        "## Batch Normalization (BN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2M-vfhR30iB"
      },
      "source": [
        "### Background:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uuDx7Le30iC"
      },
      "source": [
        "- Batch Normalization is a technique to speed up training and help make the model more stable.\n",
        "- In simple words, batch normalization is just another network layer that gets inserted between a hidden layer and the next hidden layer. Its job is to take the outputs from the first hidden layer and normalize them before passing them on as the input of the next hidden layer.\n",
        "\n",
        "- For more detailed information, you may refer to the original paper: https://arxiv.org/pdf/1502.03167.pdf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW1-jcRo30iD"
      },
      "source": [
        "### BN Algorithm:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB2crByb30iE"
      },
      "source": [
        "- Input: Values of $x$ over a mini-batch: $\\mathbf{B}$ = $\\{x_1,..., x_m\\};$\n",
        "- Output: $\\{y_i = BN_{\\gamma,\\beta}(x_i)\\}$, $\\gamma, \\beta$ are learnable parameters\n",
        "\n",
        "Normalization of the Input:\n",
        "$$\\mu_{\\mathbf{B}} = \\frac{1}{m}\\sum_{i=1}^m x_i$$\n",
        "$$\\sigma_{\\mathbf{B}}^2 = \\frac{1}{m}\\sum_{i=1}^m (x_i - \\mu_{\\mathbf{B}})^2$$\n",
        "$$\\hat{x_i} = \\frac{x_i - \\mu_{\\mathbf{B}}}{\\sqrt{\\sigma_{\\mathbf{B}}}^2 + \\epsilon}$$\n",
        "Re-scaling and Offsetting:\n",
        "$$y_i = \\gamma \\hat{x_i} + \\beta = BN_{\\gamma,\\beta}(x_i)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ep48dPG30iF"
      },
      "source": [
        "### Advantages of BN:\n",
        "1. Improves gradient flow through the network.\n",
        "2. Allows use of saturating nonlinearities and higher learning rates.\n",
        "3. Makes weights easier to initialize.\n",
        "4. Act as a form of regularization and may reduce the need for dropout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyx_CusE30iG"
      },
      "source": [
        "### Implementation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGu1ImiB30iG"
      },
      "source": [
        "- The batch normalization layer has already been implemented in many packages. You may simply call the function to build the layer. For example: torch.nn.BatchNorm2d() using pytroch package, keras.layers.BatchNormalization() using keras package.\n",
        "- The location of BN layer: Please make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZrH34MF30iH"
      },
      "source": [
        "## 1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgtGg2rl30iH"
      },
      "source": [
        "### 1.1. Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpIGKxC930iI",
        "outputId": "01d901d6-05cd-4be2-821a-90a357d806e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "170508288/170498071 [==============================] - 6s 0us/step\n",
            "shape of x_train: (50000, 32, 32, 3)\n",
            "shape of y_train: (50000, 1)\n",
            "shape of x_test: (10000, 32, 32, 3)\n",
            "shape of y_test: (10000, 1)\n",
            "number of classes: 10\n"
          ]
        }
      ],
      "source": [
        "# Load Cifar-10 Data\n",
        "# This is just an example, you may load dataset from other packages.\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "\n",
        "### If you can not load keras dataset, un-comment these two lines.\n",
        "#import ssl\n",
        "#ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('shape of x_train: ' + str(x_train.shape))\n",
        "print('shape of y_train: ' + str(y_train.shape))\n",
        "print('shape of x_test: ' + str(x_test.shape))\n",
        "print('shape of y_test: ' + str(y_test.shape))\n",
        "print('number of classes: ' + str(np.max(y_train) - np.min(y_train) + 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3xxlfwY30iK"
      },
      "source": [
        "### 1.2. One-hot encode the labels (5 points)\n",
        "\n",
        "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
        "\n",
        "1. Implement a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
        "\n",
        "2. Apply the function to ```y_train``` and ```y_test```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8mMdfug30iL",
        "outputId": "834bc688-6d69-41b9-e83a-cfe8b7622c1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of y_train_vec: (50000, 10)\n",
            "Shape of y_test_vec: (10000, 10)\n",
            "[6]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "def to_one_hot(y, num_class=10):\n",
        "    mat = np.zeros((y.shape[0],num_class))\n",
        "    for i in range(y.shape[0]):\n",
        "        mat[i][y[i][0]] = 1\n",
        "    return mat\n",
        "\n",
        "y_train_vec = to_one_hot(y_train)\n",
        "y_test_vec = to_one_hot(y_test)\n",
        "\n",
        "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
        "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
        "\n",
        "print(y_train[0])\n",
        "print(y_train_vec[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgHAbUx330iM"
      },
      "source": [
        "#### Remark: the outputs should be\n",
        "* Shape of y_train_vec: (50000, 10)\n",
        "* Shape of y_test_vec: (10000, 10)\n",
        "* [6]\n",
        "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y7ypTzy30iM"
      },
      "source": [
        "### 1.3. Randomly partition the training set to training and validation sets (5 points)\n",
        "\n",
        "Randomly partition the 50K training samples to 2 sets: \n",
        "* a training set containing 40K samples: x_tr, y_tr\n",
        "* a validation set containing 10K samples: x_val, y_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEgTY9_Q30iN",
        "outputId": "dfd3d143-241d-41a2-b4d4-77b7f2f424f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x_tr: (40000, 32, 32, 3)\n",
            "Shape of y_tr: (40000, 1)\n",
            "Shape of x_val: (10000, 32, 32, 3)\n",
            "Shape of y_val: (10000, 1)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(x_train,y_train,test_size=0.2)\n",
        "\n",
        "print('Shape of x_tr: ' + str(x_tr.shape))\n",
        "print('Shape of y_tr: ' + str(y_tr.shape))\n",
        "print('Shape of x_val: ' + str(x_val.shape))\n",
        "print('Shape of y_val: ' + str(y_val.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQc8abP930iN"
      },
      "source": [
        "## 2. Build a CNN and tune its hyper-parameters (50 points)\n",
        "\n",
        "- Build a convolutional neural network model using the below structure:\n",
        "\n",
        "- It should have a structure of: Conv - ReLU - Max Pool - ConV - ReLU - Max Pool - Dense - ReLU - Dense - Softmax\n",
        "\n",
        "- In the graph 3@32x32 means the dimension of input image, 32@30x30 means it has 32 filters and the dimension now becomes 30x30 after the convolution.\n",
        "- All convolutional layers (Conv) should have stride = 1 and no padding.\n",
        "- Max Pooling has a pool size of 2 by 2.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGijDzmq30iO"
      },
      "source": [
        "<img src=\"network.PNG\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEGTt3z630iO"
      },
      "source": [
        "- You may use the validation data to tune the hyper-parameters (e.g., learning rate, and optimization algorithm)\n",
        "- Do NOT use test data for hyper-parameter tuning!!!\n",
        "- Try to achieve a validation accuracy as high as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NISvvLtL30iO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "577d36bb-d083-4651-fcdc-35ed7bba3c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 15, 15, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 12, 12, 64)        32832     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 6, 6, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2304)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               590080    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 626,378\n",
            "Trainable params: 626,378\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Build the model\n",
        "# Base Code from Lecture 6 and TensorFlow Tutorials: https://www.tensorflow.org/tutorials/images/cnn\n",
        "\n",
        "from keras import datasets, layers, models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (4, 4), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IL-0LHIv30iP"
      },
      "outputs": [],
      "source": [
        "# Define model optimizer and loss function\n",
        "from tensorflow import optimizers\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.config.run_functions_eagerly(True)\n",
        "tf.data.experimental.enable_debug_mode()\n",
        "\n",
        "model.compile(optimizers.RMSprop(learning_rate=.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "x2oo4HBE30iP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d59ae9a-a401-42a5-c384-5ea17498f28b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "157/157 [==============================] - 17s 50ms/step - loss: 5.1778 - accuracy: 0.1845 - val_loss: 2.3749 - val_accuracy: 0.2393\n",
            "Epoch 2/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 1.9342 - accuracy: 0.3350 - val_loss: 1.9282 - val_accuracy: 0.3751\n",
            "Epoch 3/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 1.5475 - accuracy: 0.4682 - val_loss: 1.4695 - val_accuracy: 0.4844\n",
            "Epoch 4/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 1.2713 - accuracy: 0.5628 - val_loss: 1.4012 - val_accuracy: 0.5227\n",
            "Epoch 5/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 1.0739 - accuracy: 0.6366 - val_loss: 1.2782 - val_accuracy: 0.5823\n",
            "Epoch 6/50\n",
            "157/157 [==============================] - 7s 48ms/step - loss: 0.9051 - accuracy: 0.6927 - val_loss: 1.5214 - val_accuracy: 0.5113\n",
            "Epoch 7/50\n",
            "157/157 [==============================] - 7s 48ms/step - loss: 0.7543 - accuracy: 0.7443 - val_loss: 1.4290 - val_accuracy: 0.6002\n",
            "Epoch 8/50\n",
            "157/157 [==============================] - 8s 53ms/step - loss: 0.6248 - accuracy: 0.7882 - val_loss: 1.5337 - val_accuracy: 0.6004\n",
            "Epoch 9/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 0.5025 - accuracy: 0.8325 - val_loss: 1.5415 - val_accuracy: 0.6074\n",
            "Epoch 10/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 0.4009 - accuracy: 0.8645 - val_loss: 1.5116 - val_accuracy: 0.6208\n",
            "Epoch 11/50\n",
            "157/157 [==============================] - 7s 48ms/step - loss: 0.3195 - accuracy: 0.8939 - val_loss: 1.8357 - val_accuracy: 0.6214\n",
            "Epoch 12/50\n",
            "157/157 [==============================] - 7s 48ms/step - loss: 0.2568 - accuracy: 0.9154 - val_loss: 2.3223 - val_accuracy: 0.5923\n",
            "Epoch 13/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 0.2292 - accuracy: 0.9252 - val_loss: 2.0116 - val_accuracy: 0.6355\n",
            "Epoch 14/50\n",
            "157/157 [==============================] - 7s 48ms/step - loss: 0.1945 - accuracy: 0.9388 - val_loss: 2.3239 - val_accuracy: 0.6179\n",
            "Epoch 15/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 0.1574 - accuracy: 0.9489 - val_loss: 2.3555 - val_accuracy: 0.5947\n",
            "Epoch 16/50\n",
            "157/157 [==============================] - 9s 59ms/step - loss: 0.1528 - accuracy: 0.9533 - val_loss: 2.4163 - val_accuracy: 0.6371\n",
            "Epoch 17/50\n",
            "157/157 [==============================] - 11s 71ms/step - loss: 0.1376 - accuracy: 0.9589 - val_loss: 2.8110 - val_accuracy: 0.6280\n",
            "Epoch 18/50\n",
            "157/157 [==============================] - 7s 48ms/step - loss: 0.1334 - accuracy: 0.9605 - val_loss: 3.1264 - val_accuracy: 0.6348\n",
            "Epoch 19/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 0.1215 - accuracy: 0.9645 - val_loss: 3.3550 - val_accuracy: 0.5935\n",
            "Epoch 20/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 0.1045 - accuracy: 0.9677 - val_loss: 2.9877 - val_accuracy: 0.6050\n",
            "Epoch 21/50\n",
            "157/157 [==============================] - 7s 47ms/step - loss: 0.1127 - accuracy: 0.9677 - val_loss: 3.2532 - val_accuracy: 0.6277\n",
            "Epoch 22/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 0.1172 - accuracy: 0.9669 - val_loss: 3.2892 - val_accuracy: 0.6370\n",
            "Epoch 23/50\n",
            "157/157 [==============================] - 7s 48ms/step - loss: 0.1121 - accuracy: 0.9700 - val_loss: 3.6639 - val_accuracy: 0.6365\n",
            "Epoch 24/50\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 0.1034 - accuracy: 0.9719 - val_loss: 4.0867 - val_accuracy: 0.6088\n",
            "Epoch 25/50\n",
            "157/157 [==============================] - 8s 52ms/step - loss: 0.1056 - accuracy: 0.9729 - val_loss: 3.8718 - val_accuracy: 0.6221\n",
            "Epoch 26/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 0.0921 - accuracy: 0.9738 - val_loss: 4.4889 - val_accuracy: 0.6068\n",
            "Epoch 27/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 0.0917 - accuracy: 0.9752 - val_loss: 3.7382 - val_accuracy: 0.5936\n",
            "Epoch 28/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 0.0973 - accuracy: 0.9747 - val_loss: 4.0527 - val_accuracy: 0.6266\n",
            "Epoch 29/50\n",
            "157/157 [==============================] - 7s 48ms/step - loss: 0.0931 - accuracy: 0.9768 - val_loss: 4.5874 - val_accuracy: 0.6370\n",
            "Epoch 30/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 0.1042 - accuracy: 0.9749 - val_loss: 4.7337 - val_accuracy: 0.6135\n",
            "Epoch 31/50\n",
            "157/157 [==============================] - 7s 48ms/step - loss: 0.0878 - accuracy: 0.9783 - val_loss: 4.6103 - val_accuracy: 0.6202\n",
            "Epoch 32/50\n",
            "157/157 [==============================] - 7s 48ms/step - loss: 0.0932 - accuracy: 0.9776 - val_loss: 4.7366 - val_accuracy: 0.6265\n",
            "Epoch 33/50\n",
            "157/157 [==============================] - 7s 48ms/step - loss: 0.0883 - accuracy: 0.9784 - val_loss: 5.4775 - val_accuracy: 0.6154\n",
            "Epoch 34/50\n",
            "157/157 [==============================] - 7s 48ms/step - loss: 0.0953 - accuracy: 0.9766 - val_loss: 4.8493 - val_accuracy: 0.6260\n",
            "Epoch 35/50\n",
            "157/157 [==============================] - 7s 48ms/step - loss: 0.0839 - accuracy: 0.9795 - val_loss: 5.4065 - val_accuracy: 0.6368\n",
            "Epoch 36/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 0.0973 - accuracy: 0.9772 - val_loss: 5.5335 - val_accuracy: 0.6301\n",
            "Epoch 37/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 0.0863 - accuracy: 0.9805 - val_loss: 6.3205 - val_accuracy: 0.5634\n",
            "Epoch 38/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 0.0997 - accuracy: 0.9789 - val_loss: 5.4518 - val_accuracy: 0.6348\n",
            "Epoch 39/50\n",
            "157/157 [==============================] - 7s 48ms/step - loss: 0.0859 - accuracy: 0.9811 - val_loss: 5.6390 - val_accuracy: 0.6313\n",
            "Epoch 40/50\n",
            "157/157 [==============================] - 7s 48ms/step - loss: 0.0894 - accuracy: 0.9806 - val_loss: 5.2109 - val_accuracy: 0.6305\n",
            "Epoch 41/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 0.0888 - accuracy: 0.9806 - val_loss: 7.8891 - val_accuracy: 0.6220\n",
            "Epoch 42/50\n",
            "157/157 [==============================] - 7s 47ms/step - loss: 0.0928 - accuracy: 0.9814 - val_loss: 6.8127 - val_accuracy: 0.5898\n",
            "Epoch 43/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 0.0893 - accuracy: 0.9808 - val_loss: 5.8166 - val_accuracy: 0.6188\n",
            "Epoch 44/50\n",
            "157/157 [==============================] - 7s 48ms/step - loss: 0.0823 - accuracy: 0.9823 - val_loss: 5.9466 - val_accuracy: 0.6381\n",
            "Epoch 45/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 0.0801 - accuracy: 0.9834 - val_loss: 6.2488 - val_accuracy: 0.6292\n",
            "Epoch 46/50\n",
            "157/157 [==============================] - 7s 48ms/step - loss: 0.1020 - accuracy: 0.9816 - val_loss: 6.3358 - val_accuracy: 0.6366\n",
            "Epoch 47/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 0.0840 - accuracy: 0.9833 - val_loss: 6.4898 - val_accuracy: 0.6449\n",
            "Epoch 48/50\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 0.0912 - accuracy: 0.9819 - val_loss: 7.5624 - val_accuracy: 0.6147\n",
            "Epoch 49/50\n",
            "157/157 [==============================] - 7s 48ms/step - loss: 0.0902 - accuracy: 0.9825 - val_loss: 7.3786 - val_accuracy: 0.6074\n",
            "Epoch 50/50\n",
            "157/157 [==============================] - 7s 48ms/step - loss: 0.0892 - accuracy: 0.9834 - val_loss: 6.3424 - val_accuracy: 0.6321\n"
          ]
        }
      ],
      "source": [
        "# Train the model and store model parameters/loss values\n",
        "if y_tr.shape[1]!=10:\n",
        "    y_tr = to_one_hot(y_tr)\n",
        "if y_val.shape[1]!=10:\n",
        "    y_val = to_one_hot(y_val)\n",
        "\n",
        "history = model.fit(x_tr, y_tr, batch_size=256, epochs=50, validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20wZyB2B30iQ"
      },
      "source": [
        "## 3. Plot the training and validation loss curve versus epochs. (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zFEE0ckA30iQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "ef842d51-5b78-4c8c-cf63-283ae6792539"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Plot the loss curve\n",
        "import matplotlib.pyplot as plot\n",
        "%matplotlib inline\n",
        "\n",
        "tr_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(len(tr_loss))\n",
        "\n",
        "plot.plot(epochs, tr_loss, 'bo', label='Training Loss')\n",
        "plot.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "\n",
        "plot.xlabel('Epochs')\n",
        "plot.ylabel('Loss')\n",
        "plot.legend()\n",
        "plot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCWVh2ad30iQ"
      },
      "source": [
        "## 4. Train (again) and evaluate the model (5 points)\n",
        "\n",
        "- To this end, you have found the \"best\" hyper-parameters. \n",
        "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
        "- Evaluate your model on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89l4VbVN30iR"
      },
      "source": [
        "### Train the model on the entire training set\n",
        "\n",
        "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6exyJUQQ30iR"
      },
      "outputs": [],
      "source": [
        "#<Compile your model again (using the same hyper-parameters you tuned above)>\n",
        "model.compile(optimizers.RMSprop(learning_rate=.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Fv_rZQUR30iS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "947e057c-96bf-46da-982f-98856573e032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.9011 - accuracy: 0.8558\n",
            "Epoch 2/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.4466 - accuracy: 0.9007\n",
            "Epoch 3/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.2889 - accuracy: 0.9271\n",
            "Epoch 4/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.2197 - accuracy: 0.9422\n",
            "Epoch 5/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1791 - accuracy: 0.9520\n",
            "Epoch 6/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1604 - accuracy: 0.9580\n",
            "Epoch 7/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1343 - accuracy: 0.9647\n",
            "Epoch 8/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1319 - accuracy: 0.9654\n",
            "Epoch 9/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1323 - accuracy: 0.9679\n",
            "Epoch 10/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1201 - accuracy: 0.9702\n",
            "Epoch 11/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1114 - accuracy: 0.9723\n",
            "Epoch 12/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1214 - accuracy: 0.9728\n",
            "Epoch 13/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1042 - accuracy: 0.9751\n",
            "Epoch 14/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0953 - accuracy: 0.9768\n",
            "Epoch 15/50\n",
            "196/196 [==============================] - 10s 52ms/step - loss: 0.1138 - accuracy: 0.9753\n",
            "Epoch 16/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1084 - accuracy: 0.9765\n",
            "Epoch 17/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0922 - accuracy: 0.9784\n",
            "Epoch 18/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1078 - accuracy: 0.9769\n",
            "Epoch 19/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1040 - accuracy: 0.9783\n",
            "Epoch 20/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0992 - accuracy: 0.9795\n",
            "Epoch 21/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1171 - accuracy: 0.9778\n",
            "Epoch 22/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1070 - accuracy: 0.9798\n",
            "Epoch 23/50\n",
            "196/196 [==============================] - 9s 48ms/step - loss: 0.1054 - accuracy: 0.9793\n",
            "Epoch 24/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1043 - accuracy: 0.9796\n",
            "Epoch 25/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1106 - accuracy: 0.9811\n",
            "Epoch 26/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0950 - accuracy: 0.9821\n",
            "Epoch 27/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0965 - accuracy: 0.9818\n",
            "Epoch 28/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0864 - accuracy: 0.9834\n",
            "Epoch 29/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0972 - accuracy: 0.9825\n",
            "Epoch 30/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0899 - accuracy: 0.9834\n",
            "Epoch 31/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0980 - accuracy: 0.9834\n",
            "Epoch 32/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1056 - accuracy: 0.9824\n",
            "Epoch 33/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0948 - accuracy: 0.9836\n",
            "Epoch 34/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0959 - accuracy: 0.9835\n",
            "Epoch 35/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1112 - accuracy: 0.9827\n",
            "Epoch 36/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0819 - accuracy: 0.9852\n",
            "Epoch 37/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1082 - accuracy: 0.9829\n",
            "Epoch 38/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0917 - accuracy: 0.9851\n",
            "Epoch 39/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0934 - accuracy: 0.9852\n",
            "Epoch 40/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0962 - accuracy: 0.9853\n",
            "Epoch 41/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1021 - accuracy: 0.9856\n",
            "Epoch 42/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0984 - accuracy: 0.9850\n",
            "Epoch 43/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0908 - accuracy: 0.9863\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0932 - accuracy: 0.9864\n",
            "Epoch 45/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0995 - accuracy: 0.9850\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1008 - accuracy: 0.9849\n",
            "Epoch 47/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0918 - accuracy: 0.9869\n",
            "Epoch 48/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0920 - accuracy: 0.9862\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0901 - accuracy: 0.9874\n",
            "Epoch 50/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0952 - accuracy: 0.9863\n"
          ]
        }
      ],
      "source": [
        "#<Train your model on the entire training set (50K samples)>\n",
        "\n",
        "history = model.fit(x_train, y_train_vec, batch_size=256, epochs=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6ym63sN30iS"
      },
      "source": [
        "## 5. Evaluate the model on the test set (5 points)\n",
        "\n",
        "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-6e-gAxR30iS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f70d9d68-0439-40e2-b7bc-74d91c00b852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 5s 16ms/step - loss: 10.4924 - accuracy: 0.6539\n",
            "Loss: 10.492378234863281\n",
            "Accuracy: 0.6539000272750854\n"
          ]
        }
      ],
      "source": [
        "# Evaluate your model performance (testing accuracy) on testing data.\n",
        "eval = model.evaluate(x_test, y_test_vec)\n",
        "print(\"Loss: \"+str(eval[0]))\n",
        "print(\"Accuracy: \"+str(eval[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPdtnnHA30iT"
      },
      "source": [
        "## 6. Building model with new structure (25 points)\n",
        "- In this section, you can build your model with adding new layers (e.g, BN layer or dropout layer, ...).\n",
        "- If you want to regularize a ```Conv/Dense layer```, you should place a ```Dropout layer``` before the ```Conv/Dense layer```.\n",
        "- You can try to compare their loss curve and testing accuracy and analyze your findings.\n",
        "- You need to try at lease two different model structures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ukxdsGjh30iT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdbb229a-3b9a-4e49-b119-1b922a6c7ea7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 30, 30, 16)        448       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 30, 30, 16)       64        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 15, 15, 16)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 15, 15, 16)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 12, 12, 16)        4112      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 12, 12, 16)       64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 6, 6, 16)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 576)               0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 576)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               147712    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 154,970\n",
            "Trainable params: 154,906\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Model 1: Less Filters with BatchNormalization and Dropout\n",
        "modelOne = models.Sequential()\n",
        "modelOne.add(layers.Conv2D(16, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "modelOne.add(layers.BatchNormalization())\n",
        "modelOne.add(layers.MaxPooling2D((2, 2)))\n",
        "modelOne.add(layers.Dropout(.2))\n",
        "modelOne.add(layers.Conv2D(16, (4, 4), activation='relu'))\n",
        "modelOne.add(layers.BatchNormalization())\n",
        "modelOne.add(layers.MaxPooling2D((2, 2)))\n",
        "modelOne.add(layers.Flatten())\n",
        "modelOne.add(layers.Dropout(.2))\n",
        "modelOne.add(layers.Dense(256, activation='relu'))\n",
        "modelOne.add(layers.Dense(10, activation='softmax'))\n",
        "modelOne.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelOne.compile(optimizers.RMSprop(learning_rate=.001),\n",
        "                 loss='categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "historyOne = model.fit(x_train, y_train_vec, batch_size=256, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP8MiXOTqZDm",
        "outputId": "34a7d23c-a2ac-4d78-d412-0d8d03394703"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1070 - accuracy: 0.9861\n",
            "Epoch 2/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0975 - accuracy: 0.9870\n",
            "Epoch 3/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0916 - accuracy: 0.9872\n",
            "Epoch 4/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0912 - accuracy: 0.9874\n",
            "Epoch 5/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1058 - accuracy: 0.9869\n",
            "Epoch 6/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0925 - accuracy: 0.9875\n",
            "Epoch 7/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1078 - accuracy: 0.9868\n",
            "Epoch 8/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1027 - accuracy: 0.9868\n",
            "Epoch 9/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0959 - accuracy: 0.9875\n",
            "Epoch 10/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1106 - accuracy: 0.9874\n",
            "Epoch 11/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1012 - accuracy: 0.9876\n",
            "Epoch 12/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0882 - accuracy: 0.9886\n",
            "Epoch 13/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0959 - accuracy: 0.9877\n",
            "Epoch 14/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0889 - accuracy: 0.9887\n",
            "Epoch 15/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1038 - accuracy: 0.9875\n",
            "Epoch 16/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1019 - accuracy: 0.9881\n",
            "Epoch 17/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0995 - accuracy: 0.9890\n",
            "Epoch 18/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0915 - accuracy: 0.9896\n",
            "Epoch 19/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0948 - accuracy: 0.9886\n",
            "Epoch 20/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1019 - accuracy: 0.9891\n",
            "Epoch 21/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1049 - accuracy: 0.9888\n",
            "Epoch 22/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0896 - accuracy: 0.9897\n",
            "Epoch 23/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1031 - accuracy: 0.9898\n",
            "Epoch 24/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0966 - accuracy: 0.9889\n",
            "Epoch 25/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0966 - accuracy: 0.9896\n",
            "Epoch 26/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0922 - accuracy: 0.9900\n",
            "Epoch 27/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0933 - accuracy: 0.9887\n",
            "Epoch 28/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0925 - accuracy: 0.9902\n",
            "Epoch 29/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1080 - accuracy: 0.9892\n",
            "Epoch 30/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1061 - accuracy: 0.9893\n",
            "Epoch 31/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0968 - accuracy: 0.9908\n",
            "Epoch 32/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0983 - accuracy: 0.9904\n",
            "Epoch 33/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0947 - accuracy: 0.9905\n",
            "Epoch 34/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0954 - accuracy: 0.9903\n",
            "Epoch 35/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1013 - accuracy: 0.9901\n",
            "Epoch 36/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1016 - accuracy: 0.9904\n",
            "Epoch 37/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0933 - accuracy: 0.9907\n",
            "Epoch 38/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0826 - accuracy: 0.9911\n",
            "Epoch 39/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0880 - accuracy: 0.9913\n",
            "Epoch 40/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0926 - accuracy: 0.9906\n",
            "Epoch 41/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0908 - accuracy: 0.9904\n",
            "Epoch 42/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0968 - accuracy: 0.9910\n",
            "Epoch 43/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0905 - accuracy: 0.9917\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1143 - accuracy: 0.9903\n",
            "Epoch 45/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1121 - accuracy: 0.9904\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1063 - accuracy: 0.9904\n",
            "Epoch 47/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0834 - accuracy: 0.9918\n",
            "Epoch 48/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0994 - accuracy: 0.9912\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0945 - accuracy: 0.9915\n",
            "Epoch 50/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0956 - accuracy: 0.9917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model 2: More Filters with BatchNormalization and Dropout\n",
        "modelTwo = models.Sequential()\n",
        "modelTwo.add(layers.Conv2D(128, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "modelOne.add(layers.BatchNormalization())\n",
        "modelTwo.add(layers.MaxPooling2D((2, 2)))\n",
        "modelOne.add(layers.Dropout(.2))\n",
        "modelTwo.add(layers.Conv2D(128, (4, 4), activation='relu'))\n",
        "modelOne.add(layers.BatchNormalization())\n",
        "modelTwo.add(layers.MaxPooling2D((2, 2)))\n",
        "modelTwo.add(layers.Flatten())\n",
        "modelOne.add(layers.Dropout(.2))\n",
        "modelTwo.add(layers.Dense(256, activation='relu'))\n",
        "modelTwo.add(layers.Dense(10, activation='softmax'))\n",
        "modelTwo.summary()"
      ],
      "metadata": {
        "id": "8zVKSRFClXa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e787f914-fd60-40a1-f173-bbb1819855ce"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 30, 30, 128)       3584      \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 15, 15, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 12, 12, 128)       262272    \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 6, 6, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 4608)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 256)               1179904   \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,448,330\n",
            "Trainable params: 1,448,330\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelTwo.compile(optimizers.RMSprop(learning_rate=.001),\n",
        "                 loss='categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "historyTwo = model.fit(x_train, y_train_vec, batch_size=256, epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sinXuQDlq9R1",
        "outputId": "eadf0509-7630-4115-daf6-c231042bf138"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1044 - accuracy: 0.9912\n",
            "Epoch 2/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0901 - accuracy: 0.9922\n",
            "Epoch 3/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1060 - accuracy: 0.9915\n",
            "Epoch 4/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1058 - accuracy: 0.9914\n",
            "Epoch 5/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0997 - accuracy: 0.9912\n",
            "Epoch 6/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1028 - accuracy: 0.9915\n",
            "Epoch 7/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0887 - accuracy: 0.9916\n",
            "Epoch 8/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0992 - accuracy: 0.9917\n",
            "Epoch 9/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0918 - accuracy: 0.9918\n",
            "Epoch 10/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1096 - accuracy: 0.9914\n",
            "Epoch 11/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0991 - accuracy: 0.9919\n",
            "Epoch 12/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0903 - accuracy: 0.9919\n",
            "Epoch 13/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1051 - accuracy: 0.9917\n",
            "Epoch 14/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0955 - accuracy: 0.9930\n",
            "Epoch 15/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1025 - accuracy: 0.9917\n",
            "Epoch 16/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0939 - accuracy: 0.9920\n",
            "Epoch 17/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0984 - accuracy: 0.9917\n",
            "Epoch 18/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1002 - accuracy: 0.9918\n",
            "Epoch 19/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1050 - accuracy: 0.9921\n",
            "Epoch 20/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1005 - accuracy: 0.9919\n",
            "Epoch 21/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0927 - accuracy: 0.9923\n",
            "Epoch 22/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1007 - accuracy: 0.9923\n",
            "Epoch 23/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1050 - accuracy: 0.9924\n",
            "Epoch 24/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0927 - accuracy: 0.9930\n",
            "Epoch 25/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1003 - accuracy: 0.9924\n",
            "Epoch 26/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1093 - accuracy: 0.9926\n",
            "Epoch 27/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1077 - accuracy: 0.9921\n",
            "Epoch 28/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1052 - accuracy: 0.9926\n",
            "Epoch 29/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.0924 - accuracy: 0.9928\n",
            "Epoch 30/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1020 - accuracy: 0.9931\n",
            "Epoch 31/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0968 - accuracy: 0.9924\n",
            "Epoch 32/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1053 - accuracy: 0.9927\n",
            "Epoch 33/50\n",
            "196/196 [==============================] - 8s 42ms/step - loss: 0.1025 - accuracy: 0.9926\n",
            "Epoch 34/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1010 - accuracy: 0.9931\n",
            "Epoch 35/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1056 - accuracy: 0.9926\n",
            "Epoch 36/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1017 - accuracy: 0.9928\n",
            "Epoch 37/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1093 - accuracy: 0.9927\n",
            "Epoch 38/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1046 - accuracy: 0.9932\n",
            "Epoch 39/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.0958 - accuracy: 0.9931\n",
            "Epoch 40/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1060 - accuracy: 0.9928\n",
            "Epoch 41/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1059 - accuracy: 0.9926\n",
            "Epoch 42/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1048 - accuracy: 0.9928\n",
            "Epoch 43/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1090 - accuracy: 0.9929\n",
            "Epoch 44/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1042 - accuracy: 0.9929\n",
            "Epoch 45/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1096 - accuracy: 0.9928\n",
            "Epoch 46/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1128 - accuracy: 0.9928\n",
            "Epoch 47/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1126 - accuracy: 0.9926\n",
            "Epoch 48/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1157 - accuracy: 0.9929\n",
            "Epoch 49/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1108 - accuracy: 0.9927\n",
            "Epoch 50/50\n",
            "196/196 [==============================] - 8s 43ms/step - loss: 0.1040 - accuracy: 0.9933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evalOne = modelOne.evaluate(x_test, y_test_vec)\n",
        "print(\"Model One Loss: \"+str(evalOne[0]))\n",
        "print(\"Model One Accuracy: \"+str(evalOne[1]))\n",
        "evalTwo = modelTwo.evaluate(x_test, y_test_vec)\n",
        "print(\"Model Two Loss: \"+str(evalTwo[0]))\n",
        "print(\"Model Two Accuracy: \"+str(evalTwo[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0zT6eumrEUm",
        "outputId": "677f406f-1cd2-44f3-a4f1-915ddae692ca"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 6s 19ms/step - loss: 13.4405 - accuracy: 0.1088\n",
            "Model One Loss: 13.440512657165527\n",
            "Model One Accuracy: 0.1088000014424324\n",
            "313/313 [==============================] - 6s 18ms/step - loss: 29.4036 - accuracy: 0.1246\n",
            "Model Two Loss: 29.40362548828125\n",
            "Model Two Accuracy: 0.12460000067949295\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "colab": {
      "name": "Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}